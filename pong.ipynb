{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pong.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_jQ1tEQCxwRx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kruegz/pong/blob/main/pong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p62G8M_viUJp"
      },
      "source": [
        "# Playing Pong with the Actor-Critic Method\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://gym.openai.com/envs/Pong-v0/\n",
        "https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af\n",
        "https://towardsdatascience.com/getting-an-ai-to-play-atari-pong-with-deep-reinforcement-learning-47b0c56e78ae\n"
      ],
      "metadata": {
        "id": "QbCoedrbk1Oj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glLwIctHiUJq"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Import necessary packages and configure global settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBeQhPi2S4m5"
      },
      "source": [
        "%%bash\n",
        "\n",
        "# Install main packages\n",
        "pip install gym > /dev/null 2>&1\n",
        "pip install pyglet > /dev/null 2>&1\n",
        "pip install atari-py > /dev/null 2>&1\n",
        "\n",
        "# Install additional packages for visualization\n",
        "sudo apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1\n",
        "\n",
        "\n",
        "# Download and install Atari ROMs\n",
        "# https://github.com/openai/atari-py#roms\n",
        "# http://www.atarimania.com/rom_collection_archive_atari_2600_roms.html\n",
        "wget http://www.atarimania.com/roms/Roms.rar\n",
        "unrar e Roms.rar\n",
        "python -m atari_py.import_roms .\n",
        "\n",
        "export DISPLAY=localhost:0.0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def resize_frame(frame):\n",
        "    frame = frame[30:-12,5:-4]\n",
        "    frame = np.average(frame,axis = 2)\n",
        "    frame = cv2.resize(frame,(84,84),interpolation = cv2.INTER_NEAREST)\n",
        "    frame = np.array(frame,dtype = np.uint8)\n",
        "    return frame\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "class Memory():\n",
        "    def __init__(self,max_len):\n",
        "        self.max_len = max_len\n",
        "        self.frames = deque(maxlen = max_len)\n",
        "        self.actions = deque(maxlen = max_len)\n",
        "        self.rewards = deque(maxlen = max_len)\n",
        "        self.done_flags = deque(maxlen = max_len)\n",
        "\n",
        "    def add_experience(self,next_frame, next_frames_reward, next_action, next_frame_terminal):\n",
        "        self.frames.append(next_frame)\n",
        "        self.actions.append(next_action)\n",
        "        self.rewards.append(next_frames_reward)\n",
        "        self.done_flags.append(next_frame_terminal)\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def initialize_new_game(name, env, agent):\n",
        "    \"\"\"We don't want an agents past game influencing its new game, so we add in some dummy data to initialize\"\"\"\n",
        "    \n",
        "    env.reset()\n",
        "    starting_frame = resize_frame(env.step(0)[0])\n",
        "\n",
        "    dummy_action = 0\n",
        "    dummy_reward = 0\n",
        "    dummy_done = False\n",
        "    for i in range(3):\n",
        "        agent.memory.add_experience(starting_frame, dummy_reward, dummy_action, dummy_done)\n",
        "\n",
        "def make_env(name, agent):\n",
        "    env = gym.make(name)\n",
        "    return env\n",
        "\n",
        "def take_step(name, env, agent, score, debug):\n",
        "    \n",
        "    #1 and 2: Update timesteps and save weights\n",
        "    agent.total_timesteps += 1\n",
        "    if agent.total_timesteps % 50000 == 0:\n",
        "      agent.model.save_weights('recent_weights.hdf5')\n",
        "      print('\\nWeights saved!')\n",
        "\n",
        "    #3: Take action\n",
        "    next_frame, next_frames_reward, next_frame_terminal, info = env.step(agent.memory.actions[-1])\n",
        "    \n",
        "    #4: Get next state\n",
        "    next_frame = resize_frame(next_frame)\n",
        "    new_state = [agent.memory.frames[-3], agent.memory.frames[-2], agent.memory.frames[-1], next_frame]\n",
        "    new_state = np.moveaxis(new_state,0,2)/255 #We have to do this to get it into keras's goofy format of [batch_size,rows,columns,channels]\n",
        "    new_state = np.expand_dims(new_state,0) #^^^\n",
        "    \n",
        "    #5: Get next action, using next state\n",
        "    next_action = agent.get_action(new_state)\n",
        "\n",
        "    #6: If game is over, return the score\n",
        "    if next_frame_terminal:\n",
        "        agent.memory.add_experience(next_frame, next_frames_reward, next_action, next_frame_terminal)\n",
        "        return (score + next_frames_reward),True\n",
        "\n",
        "    #7: Now we add the next experience to memory\n",
        "    agent.memory.add_experience(next_frame, next_frames_reward, next_action, next_frame_terminal)\n",
        "\n",
        "    #8: If we are trying to debug this then render\n",
        "    if debug:\n",
        "        env.render()\n",
        "\n",
        "    #9: If the threshold memory is satisfied, make the agent learn from memory\n",
        "    if len(agent.memory.frames) > agent.starting_mem_len:\n",
        "        agent.learn(debug)\n",
        "\n",
        "    return (score + next_frames_reward),False\n",
        "\n",
        "def tf_take_step(name,env,agent,score, debug):\n",
        "    return tf.numpy_function(take_step, [name,env,agent,score, debug], \n",
        "                           [tf.float32, tf.int32, tf.int32])\n",
        "\n",
        "# @tf.function\n",
        "def play_episode(name, env, agent, debug = False):\n",
        "    initialize_new_game(name, env, agent)\n",
        "    done = False\n",
        "    score = 0\n",
        "    while True:\n",
        "        score,done = take_step(name,env,agent,score, debug)\n",
        "        if done:\n",
        "            break\n",
        "    return score\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential, clone_model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self,possible_actions,starting_mem_len,max_mem_len,starting_epsilon,learn_rate, starting_lives = 5, debug = False):\n",
        "        self.memory = Memory(max_mem_len)\n",
        "        self.possible_actions = possible_actions\n",
        "        self.epsilon = starting_epsilon\n",
        "        self.epsilon_decay = .9/100000\n",
        "        self.epsilon_min = .05\n",
        "        self.gamma = .95\n",
        "        self.learn_rate = learn_rate\n",
        "        self.model = self._build_model()\n",
        "        self.model_target = clone_model(self.model)\n",
        "        self.total_timesteps = 0\n",
        "        self.lives = starting_lives #this parameter does not apply to pong\n",
        "        self.starting_mem_len = starting_mem_len\n",
        "        self.learns = 0\n",
        "\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Input((84,84,4)))\n",
        "        model.add(Conv2D(filters = 32,kernel_size = (8,8),strides = 4,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
        "        model.add(Conv2D(filters = 64,kernel_size = (4,4),strides = 2,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
        "        model.add(Conv2D(filters = 64,kernel_size = (3,3),strides = 1,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512,activation = 'relu', kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
        "        model.add(Dense(len(self.possible_actions), activation = 'linear'))\n",
        "        optimizer = Adam(self.learn_rate)\n",
        "        model.compile(optimizer, loss=tf.keras.losses.Huber())\n",
        "        model.summary()\n",
        "        print('\\nAgent Initialized\\n')\n",
        "        return model\n",
        "\n",
        "    def get_action(self,state):\n",
        "        \"\"\"Explore\"\"\"\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return random.sample(self.possible_actions,1)[0]\n",
        "\n",
        "        \"\"\"Do Best Acton\"\"\"\n",
        "        a_index = np.argmax(self.model.predict(state))\n",
        "        return self.possible_actions[a_index]\n",
        "\n",
        "    def _index_valid(self,index):\n",
        "        if self.memory.done_flags[index-3] or self.memory.done_flags[index-2] or self.memory.done_flags[index-1] or self.memory.done_flags[index]:\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "\n",
        "    def learn(self,debug = False):\n",
        "        \"\"\"we want the output[a] to be R_(t+1) + Qmax_(t+1).\"\"\"\n",
        "        \"\"\"So target for taking action 1 should be [output[0], R_(t+1) + Qmax_(t+1), output[2]]\"\"\"\n",
        "\n",
        "        \"\"\"First we need 32 random valid indicies\"\"\"\n",
        "        states = []\n",
        "        next_states = []\n",
        "        actions_taken = []\n",
        "        next_rewards = []\n",
        "        next_done_flags = []\n",
        "\n",
        "        while len(states) < 32:\n",
        "            index = np.random.randint(4,len(self.memory.frames) - 1)\n",
        "            if self._index_valid(index):\n",
        "                state = [self.memory.frames[index-3], self.memory.frames[index-2], self.memory.frames[index-1], self.memory.frames[index]]\n",
        "                state = np.moveaxis(state,0,2)/255\n",
        "                next_state = [self.memory.frames[index-2], self.memory.frames[index-1], self.memory.frames[index], self.memory.frames[index+1]]\n",
        "                next_state = np.moveaxis(next_state,0,2)/255\n",
        "\n",
        "                states.append(state)\n",
        "                next_states.append(next_state)\n",
        "                actions_taken.append(self.memory.actions[index])\n",
        "                next_rewards.append(self.memory.rewards[index+1])\n",
        "                next_done_flags.append(self.memory.done_flags[index+1])\n",
        "\n",
        "        \"\"\"Now we get the ouputs from our model, and the target model. We need this for our target in the error function\"\"\"\n",
        "        labels = self.model.predict(np.array(states))\n",
        "        next_state_values = self.model_target.predict(np.array(next_states))\n",
        "        \n",
        "        \"\"\"Now we define our labels, or what the output should have been\n",
        "           We want the output[action_taken] to be R_(t+1) + Qmax_(t+1) \"\"\"\n",
        "        for i in range(32):\n",
        "            action = self.possible_actions.index(actions_taken[i])\n",
        "            labels[i][action] = next_rewards[i] + (not next_done_flags[i]) * self.gamma * max(next_state_values[i])\n",
        "\n",
        "        \"\"\"Train our model using the states and outputs generated\"\"\"\n",
        "        self.model.fit(np.array(states),labels,batch_size = 32, epochs = 1, verbose = 0)\n",
        "\n",
        "        \"\"\"Decrease epsilon and update how many times our agent has learned\"\"\"\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon -= self.epsilon_decay\n",
        "        self.learns += 1\n",
        "        \n",
        "        \"\"\"Every 10000 learned, copy our model weights to our target model\"\"\"\n",
        "        if self.learns % 10000 == 0:\n",
        "            self.model_target.set_weights(self.model.get_weights())\n",
        "            print('\\nTarget model updated')\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "\n",
        "name = 'Pong-v0'\n",
        "\n",
        "agent = Agent(possible_actions=[0,2,3],starting_mem_len=50000,max_mem_len=750000,starting_epsilon = 1, learn_rate = .00025)\n",
        "env = make_env(name,agent)\n",
        "\n",
        "last_100_avg = [-21]\n",
        "scores = deque(maxlen = 100)\n",
        "max_score = -21\n",
        "\n",
        "\"\"\" If testing:\n",
        "agent.model.load_weights('recent_weights.hdf5')\n",
        "agent.model_target.load_weights('recent_weights.hdf5')\n",
        "agent.epsilon = 0.0\n",
        "\"\"\"\n",
        "\n",
        "env.reset()\n",
        "\n",
        "for i in range(1000000):\n",
        "    timesteps = agent.total_timesteps\n",
        "    timee = time.time()\n",
        "    score = play_episode(name, env, agent, debug = False) #set debug to true for rendering\n",
        "    scores.append(score)\n",
        "    if score > max_score:\n",
        "        max_score = score\n",
        "\n",
        "    print('\\nEpisode: ' + str(i))\n",
        "    print('Steps: ' + str(agent.total_timesteps - timesteps))\n",
        "    print('Duration: ' + str(time.time() - timee))\n",
        "    print('Score: ' + str(score))\n",
        "    print('Max Score: ' + str(max_score))\n",
        "    print('Epsilon: ' + str(agent.epsilon))\n",
        "\n",
        "    if i%100==0 and i!=0:\n",
        "        last_100_avg.append(sum(scores)/len(scores))\n",
        "        plt.plot(np.arange(0,i+1,100),last_100_avg)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmaDSWhLNS59",
        "outputId": "4ddbd02f-1000-4289-91a7-c0a01062c487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 20, 20, 32)        8224      \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 9, 9, 64)          32832     \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 7, 7, 64)          36928     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 3136)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               1606144   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3)                 1539      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,685,667\n",
            "Trainable params: 1,685,667\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "Agent Initialized\n",
            "\n",
            "\n",
            "Episode: 0\n",
            "Steps: 1403\n",
            "Duration: 4.830768346786499\n",
            "Score: -21.0\n",
            "Max Score: -21\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 1\n",
            "Steps: 1169\n",
            "Duration: 4.057057619094849\n",
            "Score: -21.0\n",
            "Max Score: -21\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 2\n",
            "Steps: 1491\n",
            "Duration: 4.7068586349487305\n",
            "Score: -21.0\n",
            "Max Score: -21\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 3\n",
            "Steps: 1110\n",
            "Duration: 3.0720486640930176\n",
            "Score: -21.0\n",
            "Max Score: -21\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 4\n",
            "Steps: 1326\n",
            "Duration: 2.917128801345825\n",
            "Score: -21.0\n",
            "Max Score: -21\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 5\n",
            "Steps: 1571\n",
            "Duration: 2.4422569274902344\n",
            "Score: -21.0\n",
            "Max Score: -21\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 6\n",
            "Steps: 1521\n",
            "Duration: 2.3520658016204834\n",
            "Score: -19.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 7\n",
            "Steps: 1104\n",
            "Duration: 1.7389836311340332\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 8\n",
            "Steps: 1253\n",
            "Duration: 1.925736665725708\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 9\n",
            "Steps: 1176\n",
            "Duration: 1.8469367027282715\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 10\n",
            "Steps: 1276\n",
            "Duration: 2.0099949836730957\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 11\n",
            "Steps: 1109\n",
            "Duration: 1.717273235321045\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 12\n",
            "Steps: 1391\n",
            "Duration: 2.1642651557922363\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 13\n",
            "Steps: 1432\n",
            "Duration: 2.2212915420532227\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 14\n",
            "Steps: 1587\n",
            "Duration: 2.457735300064087\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 15\n",
            "Steps: 1266\n",
            "Duration: 2.009486198425293\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 16\n",
            "Steps: 1100\n",
            "Duration: 1.7198865413665771\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 17\n",
            "Steps: 1092\n",
            "Duration: 1.7094497680664062\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 18\n",
            "Steps: 1323\n",
            "Duration: 2.0544984340667725\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 19\n",
            "Steps: 1216\n",
            "Duration: 1.8997418880462646\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 20\n",
            "Steps: 1413\n",
            "Duration: 2.190110445022583\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 21\n",
            "Steps: 1301\n",
            "Duration: 2.009782552719116\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 22\n",
            "Steps: 1257\n",
            "Duration: 1.9488537311553955\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 23\n",
            "Steps: 1217\n",
            "Duration: 1.8664608001708984\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 24\n",
            "Steps: 1670\n",
            "Duration: 3.4446771144866943\n",
            "Score: -19.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 25\n",
            "Steps: 1261\n",
            "Duration: 1.926149606704712\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 26\n",
            "Steps: 1108\n",
            "Duration: 1.681248664855957\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 27\n",
            "Steps: 1395\n",
            "Duration: 2.126871109008789\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 28\n",
            "Steps: 1396\n",
            "Duration: 2.1129093170166016\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 29\n",
            "Steps: 1180\n",
            "Duration: 1.792611837387085\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 30\n",
            "Steps: 1447\n",
            "Duration: 2.2067863941192627\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 31\n",
            "Steps: 1088\n",
            "Duration: 1.650895357131958\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 32\n",
            "Steps: 1262\n",
            "Duration: 1.9107623100280762\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 33\n",
            "Steps: 1178\n",
            "Duration: 1.8227477073669434\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 34\n",
            "Steps: 1115\n",
            "Duration: 1.7180533409118652\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 35\n",
            "Steps: 1180\n",
            "Duration: 1.8089087009429932\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 36\n",
            "Steps: 1309\n",
            "Duration: 1.9928574562072754\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 37\n",
            "Steps: 1338\n",
            "Duration: 2.0674264430999756\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Weights saved!\n",
            "\n",
            "Episode: 38\n",
            "Steps: 1522\n",
            "Duration: 170.6084246635437\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 0.9939789999999754\n",
            "\n",
            "Episode: 39\n",
            "Steps: 1690\n",
            "Duration: 430.8504431247711\n",
            "Score: -19.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 0.9787779999999133\n",
            "\n",
            "Episode: 40\n",
            "Steps: 1337\n",
            "Duration: 341.95294165611267\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 0.9667539999998642\n",
            "\n",
            "Episode: 41\n",
            "Steps: 1475\n",
            "Duration: 380.55098056793213\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 0.95348799999981\n",
            "\n",
            "Episode: 42\n",
            "Steps: 1525\n",
            "Duration: 391.83900356292725\n",
            "Score: -19.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 0.939771999999754\n",
            "\n",
            "Episode: 43\n",
            "Steps: 1267\n",
            "Duration: 327.6377317905426\n",
            "Score: -19.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 0.9283779999997075\n",
            "\n",
            "Episode: 44\n",
            "Steps: 1503\n",
            "Duration: 394.81334352493286\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 0.9148599999996523\n",
            "\n",
            "Target model updated\n",
            "\n",
            "Episode: 45\n",
            "Steps: 1382\n",
            "Duration: 361.733606338501\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 0.9024309999996015\n",
            "\n",
            "Episode: 46\n",
            "Steps: 1012\n",
            "Duration: 266.88785791397095\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 0.8933319999995644\n",
            "\n",
            "Episode: 47\n",
            "Steps: 1300\n",
            "Duration: 338.50309777259827\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 0.8816409999995166\n",
            "\n",
            "Episode: 48\n",
            "Steps: 1668\n",
            "Duration: 434.4183900356293\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 0.8666379999994553\n",
            "\n",
            "Episode: 49\n",
            "Steps: 1171\n",
            "Duration: 305.10317516326904\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 0.8561079999994123\n",
            "\n",
            "Episode: 50\n",
            "Steps: 1105\n",
            "Duration: 289.52302646636963\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 0.8461719999993718\n",
            "\n",
            "Episode: 51\n",
            "Steps: 1162\n",
            "Duration: 301.16761684417725\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 0.8357229999993291\n",
            "\n",
            "Episode: 52\n",
            "Steps: 1581\n",
            "Duration: 412.5962817668915\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 0.821502999999271\n",
            "\n",
            "Target model updated\n",
            "\n",
            "Episode: 53\n",
            "Steps: 1263\n",
            "Duration: 331.4553520679474\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 0.8101449999992246\n",
            "\n",
            "Episode: 54\n",
            "Steps: 1943\n",
            "Duration: 513.6766774654388\n",
            "Score: -17.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.7926669999991532\n",
            "\n",
            "Episode: 55\n",
            "Steps: 1248\n",
            "Duration: 330.8619952201843\n",
            "Score: -21.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.7814439999991074\n",
            "\n",
            "Episode: 56\n",
            "Steps: 1528\n",
            "Duration: 402.6136646270752\n",
            "Score: -20.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.7677009999990513\n",
            "\n",
            "Episode: 57\n",
            "Steps: 1422\n",
            "Duration: 378.20521664619446\n",
            "Score: -21.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.754911999998999\n",
            "\n",
            "Episode: 58\n",
            "Steps: 1344\n",
            "Duration: 359.1206486225128\n",
            "Score: -21.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.7428249999989497\n",
            "\n",
            "Episode: 59\n",
            "Steps: 1335\n",
            "Duration: 362.70653319358826\n",
            "Score: -21.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.7308189999989007\n",
            "\n",
            "Target model updated\n",
            "\n",
            "Episode: 60\n",
            "Steps: 1390\n",
            "Duration: 377.92895007133484\n",
            "Score: -20.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.7183179999988496\n",
            "\n",
            "Episode: 61\n",
            "Steps: 1171\n",
            "Duration: 319.53934144973755\n",
            "Score: -21.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.7077879999988066\n",
            "\n",
            "Episode: 62\n",
            "Steps: 1738\n",
            "Duration: 472.99549412727356\n",
            "Score: -20.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.6921549999987427\n",
            "\n",
            "Episode: 63\n",
            "Steps: 1423\n",
            "Duration: 387.6057288646698\n",
            "Score: -21.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.6793569999986905\n",
            "\n",
            "Episode: 64\n",
            "Steps: 1742\n",
            "Duration: 473.5858337879181\n",
            "Score: -19.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.6636879999986265\n",
            "\n",
            "Episode: 65\n",
            "Steps: 1196\n",
            "Duration: 324.5978343486786\n",
            "Score: -21.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.6529329999985826\n",
            "\n",
            "Episode: 66\n",
            "Steps: 1094\n",
            "Duration: 299.80999541282654\n",
            "Score: -21.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.6430959999985424\n",
            "\n",
            "Target model updated\n",
            "\n",
            "Episode: 67\n",
            "Steps: 1100\n",
            "Duration: 304.04707527160645\n",
            "Score: -21.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.633204999998502\n",
            "\n",
            "Episode: 68\n",
            "Steps: 1595\n",
            "Duration: 448.7861216068268\n",
            "Score: -19.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.6188589999984434\n",
            "\n",
            "Episode: 69\n",
            "Steps: 1305\n",
            "Duration: 369.99503540992737\n",
            "Score: -20.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.6071229999983955\n",
            "\n",
            "Episode: 70\n",
            "Steps: 1314\n",
            "Duration: 369.64072465896606\n",
            "Score: -20.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.5953059999983472\n",
            "\n",
            "Episode: 71\n",
            "Steps: 1484\n",
            "Duration: 421.35886907577515\n",
            "Score: -20.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.5819589999982927\n",
            "\n",
            "Episode: 72\n",
            "Steps: 1297\n",
            "Duration: 364.2922465801239\n",
            "Score: -20.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.5702949999982451\n",
            "\n",
            "Episode: 73\n",
            "Steps: 1180\n",
            "Duration: 331.8997962474823\n",
            "Score: -21.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.5596839999982017\n",
            "\n",
            "Target model updated\n",
            "\n",
            "Weights saved!\n",
            "\n",
            "Episode: 74\n",
            "Steps: 1271\n",
            "Duration: 360.2099013328552\n",
            "Score: -21.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.548253999998155\n",
            "\n",
            "Episode: 75\n",
            "Steps: 1758\n",
            "Duration: 508.7304162979126\n",
            "Score: -19.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.5324409999980905\n",
            "\n",
            "Episode: 76\n",
            "Steps: 1178\n",
            "Duration: 330.5286250114441\n",
            "Score: -21.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.5218479999980472\n",
            "\n",
            "Episode: 77\n",
            "Steps: 1112\n",
            "Duration: 316.6477999687195\n",
            "Score: -21.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.5118489999980064\n",
            "\n",
            "Episode: 78\n",
            "Steps: 1245\n",
            "Duration: 354.59080505371094\n",
            "Score: -21.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.5006529999979606\n",
            "\n",
            "Episode: 79\n",
            "Steps: 1233\n",
            "Duration: 348.47354459762573\n",
            "Score: -20.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.48956499999797976\n",
            "\n",
            "Episode: 80\n",
            "Steps: 1628\n",
            "Duration: 462.61259484291077\n",
            "Score: -20.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.47492199999801027\n",
            "\n",
            "Target model updated\n",
            "\n",
            "Episode: 81\n",
            "Steps: 1702\n",
            "Duration: 483.4916820526123\n",
            "Score: -20.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.45961299999804217\n",
            "\n",
            "Episode: 82\n",
            "Steps: 1475\n",
            "Duration: 420.3125693798065\n",
            "Score: -20.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.4463469999980698\n",
            "\n",
            "Episode: 83\n",
            "Steps: 1254\n",
            "Duration: 358.91446447372437\n",
            "Score: -21.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.4350699999980933\n",
            "\n",
            "Episode: 84\n",
            "Steps: 1505\n",
            "Duration: 432.7464756965637\n",
            "Score: -21.0\n",
            "Max Score: -17.0\n",
            "Epsilon: 0.4215339999981215\n"
          ]
        }
      ]
    }
  ]
}