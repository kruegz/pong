{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kruegz/pong/blob/actor_critic/pong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p62G8M_viUJp"
      },
      "source": [
        "# Playing Pong with the Actor-Critic Method\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FPe-n4ZLa7V",
        "outputId": "3c9a6a33-149e-4ba2-92c0-acdac12741da"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WBeQhPi2S4m5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69e0040b-3c8a-4e15-d36b-0cd71b390e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from Roms.rar\n",
            "\n",
            "Extracting  HC ROMS.zip                                                  \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Extracting  ROMS.zip                                                     \b\b\b\b 74%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n",
            "copying adventure.bin from HC ROMS/BY ALPHABET (PAL)/A-G/Adventure (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying air_raid.bin from HC ROMS/BY ALPHABET (PAL)/A-G/Air Raid (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying alien.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Alien.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying crazy_climber.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Crazy Climber.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying elevator_action.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Elevator Action (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying gravitar.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Gravitar.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying keystone_kapers.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Keystone Kapers (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying king_kong.bin from HC ROMS/BY ALPHABET (PAL)/H-R/King Kong (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying laser_gates.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Laser Gates (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying mr_do.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Mr. Do! (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying pacman.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Pac-Man (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying jamesbond.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/James Bond 007.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying koolaid.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Kool-Aid Man.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying krull.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Krull.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying montezuma_revenge.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Montezuma's Revenge - Featuring Panama Joe.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying star_gunner.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Stargunner.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying time_pilot.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Time Pilot.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying up_n_down.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Up 'n Down.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying sir_lancelot.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/Sir Lancelot (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n",
            "copying amidar.bin from HC ROMS/BY ALPHABET/A-G/Amidar.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying asteroids.bin from HC ROMS/BY ALPHABET/A-G/Asteroids [no copyright].bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying atlantis.bin from HC ROMS/BY ALPHABET/A-G/Atlantis.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying bank_heist.bin from HC ROMS/BY ALPHABET/A-G/Bank Heist.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
            "copying battle_zone.bin from HC ROMS/BY ALPHABET/A-G/Battlezone.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying beam_rider.bin from HC ROMS/BY ALPHABET/A-G/Beamrider.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying berzerk.bin from HC ROMS/BY ALPHABET/A-G/Berzerk.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying bowling.bin from HC ROMS/BY ALPHABET/A-G/Bowling.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying boxing.bin from HC ROMS/BY ALPHABET/A-G/Boxing.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying breakout.bin from HC ROMS/BY ALPHABET/A-G/Breakout - Breakaway IV.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying carnival.bin from HC ROMS/BY ALPHABET/A-G/Carnival.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying centipede.bin from HC ROMS/BY ALPHABET/A-G/Centipede.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying chopper_command.bin from HC ROMS/BY ALPHABET/A-G/Chopper Command.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying defender.bin from HC ROMS/BY ALPHABET/A-G/Defender.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying demon_attack.bin from HC ROMS/BY ALPHABET/A-G/Demon Attack.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying donkey_kong.bin from HC ROMS/BY ALPHABET/A-G/Donkey Kong.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying double_dunk.bin from HC ROMS/BY ALPHABET/A-G/Double Dunk.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying enduro.bin from HC ROMS/BY ALPHABET/A-G/Enduro.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying fishing_derby.bin from HC ROMS/BY ALPHABET/A-G/Fishing Derby.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying freeway.bin from HC ROMS/BY ALPHABET/A-G/Freeway.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying frogger.bin from HC ROMS/BY ALPHABET/A-G/Frogger.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying frostbite.bin from HC ROMS/BY ALPHABET/A-G/Frostbite.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying galaxian.bin from HC ROMS/BY ALPHABET/A-G/Galaxian.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying gopher.bin from HC ROMS/BY ALPHABET/A-G/Gopher.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying hero.bin from HC ROMS/BY ALPHABET/H-R/H.E.R.O..bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying ice_hockey.bin from HC ROMS/BY ALPHABET/H-R/Ice Hockey.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying journey_escape.bin from HC ROMS/BY ALPHABET/H-R/Journey Escape.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying kaboom.bin from HC ROMS/BY ALPHABET/H-R/Kaboom!.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying kangaroo.bin from HC ROMS/BY ALPHABET/H-R/Kangaroo.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying kung_fu_master.bin from HC ROMS/BY ALPHABET/H-R/Kung-Fu Master.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying lost_luggage.bin from HC ROMS/BY ALPHABET/H-R/Lost Luggage [no opening scene].bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying ms_pacman.bin from HC ROMS/BY ALPHABET/H-R/Ms. Pac-Man.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying name_this_game.bin from HC ROMS/BY ALPHABET/H-R/Name This Game.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying phoenix.bin from HC ROMS/BY ALPHABET/H-R/Phoenix.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying pitfall.bin from HC ROMS/BY ALPHABET/H-R/Pitfall! - Pitfall Harry's Jungle Adventure.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying pooyan.bin from HC ROMS/BY ALPHABET/H-R/Pooyan.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying private_eye.bin from HC ROMS/BY ALPHABET/H-R/Private Eye.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying qbert.bin from HC ROMS/BY ALPHABET/H-R/Q-bert.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying riverraid.bin from HC ROMS/BY ALPHABET/H-R/River Raid.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying road_runner.bin from patched version of HC ROMS/BY ALPHABET/H-R/Road Runner.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying robotank.bin from HC ROMS/BY ALPHABET/H-R/Robot Tank.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying seaquest.bin from HC ROMS/BY ALPHABET/S-Z/Seaquest.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying skiing.bin from HC ROMS/BY ALPHABET/S-Z/Skiing.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying solaris.bin from HC ROMS/BY ALPHABET/S-Z/Solaris.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying space_invaders.bin from HC ROMS/BY ALPHABET/S-Z/Space Invaders.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying surround.bin from HC ROMS/BY ALPHABET/S-Z/Surround - Chase.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying tennis.bin from HC ROMS/BY ALPHABET/S-Z/Tennis.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying trondead.bin from HC ROMS/BY ALPHABET/S-Z/TRON - Deadly Discs.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying tutankham.bin from HC ROMS/BY ALPHABET/S-Z/Tutankham.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying venture.bin from HC ROMS/BY ALPHABET/S-Z/Venture.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying pong.bin from HC ROMS/BY ALPHABET/S-Z/Video Olympics - Pong Sports.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying video_pinball.bin from HC ROMS/BY ALPHABET/S-Z/Video Pinball - Arcade Pinball.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying wizard_of_wor.bin from HC ROMS/BY ALPHABET/S-Z/Wizard of Wor.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying yars_revenge.bin from HC ROMS/BY ALPHABET/S-Z/Yars' Revenge.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying zaxxon.bin from HC ROMS/BY ALPHABET/S-Z/Zaxxon.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n",
            "copying assault.bin from HC ROMS/NTSC VERSIONS OF PAL ORIGINALS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying asterix.bin from ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "--2022-02-09 15:10:14--  http://www.atarimania.com/roms/Roms.rar\n",
            "Resolving www.atarimania.com (www.atarimania.com)... 195.154.81.199\n",
            "Connecting to www.atarimania.com (www.atarimania.com)|195.154.81.199|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11128004 (11M) [application/x-rar-compressed]\n",
            "Saving to: ‘Roms.rar’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  0%  208K 52s\n",
            "    50K .......... .......... .......... .......... ..........  0%  209K 52s\n",
            "   100K .......... .......... .......... .......... ..........  1%  622K 40s\n",
            "   150K .......... .......... .......... .......... ..........  1%  314K 38s\n",
            "   200K .......... .......... .......... .......... ..........  2% 52.6M 31s\n",
            "   250K .......... .......... .......... .......... ..........  2%  627K 28s\n",
            "   300K .......... .......... .......... .......... ..........  3%  628K 26s\n",
            "   350K .......... .......... .......... .......... ..........  3%  627K 25s\n",
            "   400K .......... .......... .......... .......... ..........  4%  624K 24s\n",
            "   450K .......... .......... .......... .......... ..........  4%  228M 22s\n",
            "   500K .......... .......... .......... .......... ..........  5%  627K 21s\n",
            "   550K .......... .......... .......... .......... ..........  5%  627K 21s\n",
            "   600K .......... .......... .......... .......... ..........  5%  621K 20s\n",
            "   650K .......... .......... .......... .......... ..........  6%  290M 19s\n",
            "   700K .......... .......... .......... .......... ..........  6%  625K 18s\n",
            "   750K .......... .......... .......... .......... ..........  7%  625K 18s\n",
            "   800K .......... .......... .......... .......... ..........  7%  624K 18s\n",
            "   850K .......... .......... .......... .......... ..........  8%  626K 18s\n",
            "   900K .......... .......... .......... .......... ..........  8%  231M 17s\n",
            "   950K .......... .......... .......... .......... ..........  9%  626K 17s\n",
            "  1000K .......... .......... .......... .......... ..........  9%  625K 16s\n",
            "  1050K .......... .......... .......... .......... .......... 10%  623K 16s\n",
            "  1100K .......... .......... .......... .......... .......... 10%  276M 16s\n",
            "  1150K .......... .......... .......... .......... .......... 11%  624K 15s\n",
            "  1200K .......... .......... .......... .......... .......... 11%  625K 15s\n",
            "  1250K .......... .......... .......... .......... .......... 11%  624K 15s\n",
            "  1300K .......... .......... .......... .......... .......... 12%  628K 15s\n",
            "  1350K .......... .......... .......... .......... .......... 12%  155M 15s\n",
            "  1400K .......... .......... .......... .......... .......... 13%  624K 15s\n",
            "  1450K .......... .......... .......... .......... .......... 13%  624K 14s\n",
            "  1500K .......... .......... .......... .......... .......... 14%  623K 14s\n",
            "  1550K .......... .......... .......... .......... .......... 14%  188M 14s\n",
            "  1600K .......... .......... .......... .......... .......... 15%  622K 14s\n",
            "  1650K .......... .......... .......... .......... .......... 15%  624K 14s\n",
            "  1700K .......... .......... .......... .......... .......... 16%  623K 14s\n",
            "  1750K .......... .......... .......... .......... .......... 16%  624K 14s\n",
            "  1800K .......... .......... .......... .......... .......... 17%  154M 13s\n",
            "  1850K .......... .......... .......... .......... .......... 17%  626K 13s\n",
            "  1900K .......... .......... .......... .......... .......... 17%  625K 13s\n",
            "  1950K .......... .......... .......... .......... .......... 18%  624K 13s\n",
            "  2000K .......... .......... .......... .......... .......... 18%  624K 13s\n",
            "  2050K .......... .......... .......... .......... .......... 19%  201M 13s\n",
            "  2100K .......... .......... .......... .......... .......... 19%  626K 13s\n",
            "  2150K .......... .......... .......... .......... .......... 20%  618K 13s\n",
            "  2200K .......... .......... .......... .......... .......... 20%  625K 13s\n",
            "  2250K .......... .......... .......... .......... .......... 21%  422M 12s\n",
            "  2300K .......... .......... .......... .......... .......... 21%  623K 12s\n",
            "  2350K .......... .......... .......... .......... .......... 22%  628K 12s\n",
            "  2400K .......... .......... .......... .......... .......... 22%  624K 12s\n",
            "  2450K .......... .......... .......... .......... .......... 23%  626K 12s\n",
            "  2500K .......... .......... .......... .......... .......... 23%  135M 12s\n",
            "  2550K .......... .......... .......... .......... .......... 23%  625K 12s\n",
            "  2600K .......... .......... .......... .......... .......... 24%  621K 12s\n",
            "  2650K .......... .......... .......... .......... .......... 24%  628K 12s\n",
            "  2700K .......... .......... .......... .......... .......... 25%  121M 11s\n",
            "  2750K .......... .......... .......... .......... .......... 25%  620K 11s\n",
            "  2800K .......... .......... .......... .......... .......... 26%  624K 11s\n",
            "  2850K .......... .......... .......... .......... .......... 26%  624K 11s\n",
            "  2900K .......... .......... .......... .......... .......... 27%  628K 11s\n",
            "  2950K .......... .......... .......... .......... .......... 27%  126M 11s\n",
            "  3000K .......... .......... .......... .......... .......... 28%  622K 11s\n",
            "  3050K .......... .......... .......... .......... .......... 28%  625K 11s\n",
            "  3100K .......... .......... .......... .......... .......... 28%  623K 11s\n",
            "  3150K .......... .......... .......... .......... .......... 29%  393M 11s\n",
            "  3200K .......... .......... .......... .......... .......... 29%  623K 11s\n",
            "  3250K .......... .......... .......... .......... .......... 30%  622K 10s\n",
            "  3300K .......... .......... .......... .......... .......... 30%  623K 10s\n",
            "  3350K .......... .......... .......... .......... .......... 31%  623K 10s\n",
            "  3400K .......... .......... .......... .......... .......... 31%  144M 10s\n",
            "  3450K .......... .......... .......... .......... .......... 32%  627K 10s\n",
            "  3500K .......... .......... .......... .......... .......... 32%  622K 10s\n",
            "  3550K .......... .......... .......... .......... .......... 33%  626K 10s\n",
            "  3600K .......... .......... .......... .......... .......... 33%  627K 10s\n",
            "  3650K .......... .......... .......... .......... .......... 34%  122M 10s\n",
            "  3700K .......... .......... .......... .......... .......... 34%  624K 10s\n",
            "  3750K .......... .......... .......... .......... .......... 34%  624K 10s\n",
            "  3800K .......... .......... .......... .......... .......... 35%  625K 10s\n",
            "  3850K .......... .......... .......... .......... .......... 35%  320M 9s\n",
            "  3900K .......... .......... .......... .......... .......... 36%  624K 9s\n",
            "  3950K .......... .......... .......... .......... .......... 36%  626K 9s\n",
            "  4000K .......... .......... .......... .......... .......... 37%  625K 9s\n",
            "  4050K .......... .......... .......... .......... .......... 37%  626K 9s\n",
            "  4100K .......... .......... .......... .......... .......... 38% 53.8M 9s\n",
            "  4150K .......... .......... .......... .......... .......... 38%  627K 9s\n",
            "  4200K .......... .......... .......... .......... .......... 39%  625K 9s\n",
            "  4250K .......... .......... .......... .......... .......... 39%  621K 9s\n",
            "  4300K .......... .......... .......... .......... .......... 40%  111M 9s\n",
            "  4350K .......... .......... .......... .......... .......... 40%  627K 9s\n",
            "  4400K .......... .......... .......... .......... .......... 40%  621K 9s\n",
            "  4450K .......... .......... .......... .......... .......... 41%  624K 9s\n",
            "  4500K .......... .......... .......... .......... .......... 41%  630K 9s\n",
            "  4550K .......... .......... .......... .......... .......... 42% 81.1M 8s\n",
            "  4600K .......... .......... .......... .......... .......... 42%  625K 8s\n",
            "  4650K .......... .......... .......... .......... .......... 43%  624K 8s\n",
            "  4700K .......... .......... .......... .......... .......... 43%  629K 8s\n",
            "  4750K .......... .......... .......... .......... .......... 44%  174M 8s\n",
            "  4800K .......... .......... .......... .......... .......... 44%  627K 8s\n",
            "  4850K .......... .......... .......... .......... .......... 45%  625K 8s\n",
            "  4900K .......... .......... .......... .......... .......... 45%  627K 8s\n",
            "  4950K .......... .......... .......... .......... .......... 46%  626K 8s\n",
            "  5000K .......... .......... .......... .......... .......... 46%  113M 8s\n",
            "  5050K .......... .......... .......... .......... .......... 46%  624K 8s\n",
            "  5100K .......... .......... .......... .......... .......... 47%  622K 8s\n",
            "  5150K .......... .......... .......... .......... .......... 47%  623K 8s\n",
            "  5200K .......... .......... .......... .......... .......... 48%  628K 8s\n",
            "  5250K .......... .......... .......... .......... .......... 48%  123M 7s\n",
            "  5300K .......... .......... .......... .......... .......... 49%  621K 7s\n",
            "  5350K .......... .......... .......... .......... .......... 49%  623K 7s\n",
            "  5400K .......... .......... .......... .......... .......... 50%  624K 7s\n",
            "  5450K .......... .......... .......... .......... .......... 50%  312M 7s\n",
            "  5500K .......... .......... .......... .......... .......... 51%  627K 7s\n",
            "  5550K .......... .......... .......... .......... .......... 51%  625K 7s\n",
            "  5600K .......... .......... .......... .......... .......... 51%  626K 7s\n",
            "  5650K .......... .......... .......... .......... .......... 52%  622K 7s\n",
            "  5700K .......... .......... .......... .......... .......... 52%  214M 7s\n",
            "  5750K .......... .......... .......... .......... .......... 53%  622K 7s\n",
            "  5800K .......... .......... .......... .......... .......... 53%  622K 7s\n",
            "  5850K .......... .......... .......... .......... .......... 54%  623K 7s\n",
            "  5900K .......... .......... .......... .......... .......... 54%  248M 6s\n",
            "  5950K .......... .......... .......... .......... .......... 55%  624K 6s\n",
            "  6000K .......... .......... .......... .......... .......... 55%  626K 6s\n",
            "  6050K .......... .......... .......... .......... .......... 56%  622K 6s\n",
            "  6100K .......... .......... .......... .......... .......... 56%  628K 6s\n",
            "  6150K .......... .......... .......... .......... .......... 57% 66.7M 6s\n",
            "  6200K .......... .......... .......... .......... .......... 57%  627K 6s\n",
            "  6250K .......... .......... .......... .......... .......... 57%  624K 6s\n",
            "  6300K .......... .......... .......... .......... .......... 58%  629K 6s\n",
            "  6350K .......... .......... .......... .......... .......... 58%  406M 6s\n",
            "  6400K .......... .......... .......... .......... .......... 59%  625K 6s\n",
            "  6450K .......... .......... .......... .......... .......... 59%  623K 6s\n",
            "  6500K .......... .......... .......... .......... .......... 60%  626K 6s\n",
            "  6550K .......... .......... .......... .......... .......... 60%  625K 6s\n",
            "  6600K .......... .......... .......... .......... .......... 61%  131M 6s\n",
            "  6650K .......... .......... .......... .......... .......... 61%  621K 5s\n",
            "  6700K .......... .......... .......... .......... .......... 62%  628K 5s\n",
            "  6750K .......... .......... .......... .......... .......... 62%  623K 5s\n",
            "  6800K .......... .......... .......... .......... .......... 63%  627K 5s\n",
            "  6850K .......... .......... .......... .......... .......... 63% 82.3M 5s\n",
            "  6900K .......... .......... .......... .......... .......... 63%  624K 5s\n",
            "  6950K .......... .......... .......... .......... .......... 64%  627K 5s\n",
            "  7000K .......... .......... .......... .......... .......... 64%  622K 5s\n",
            "  7050K .......... .......... .......... .......... .......... 65%  218M 5s\n",
            "  7100K .......... .......... .......... .......... .......... 65%  623K 5s\n",
            "  7150K .......... .......... .......... .......... .......... 66%  625K 5s\n",
            "  7200K .......... .......... .......... .......... .......... 66%  624K 5s\n",
            "  7250K .......... .......... .......... .......... .......... 67%  622K 5s\n",
            "  7300K .......... .......... .......... .......... .......... 67% 86.7M 5s\n",
            "  7350K .......... .......... .......... .......... .......... 68%  624K 5s\n",
            "  7400K .......... .......... .......... .......... .......... 68%  625K 4s\n",
            "  7450K .......... .......... .......... .......... .......... 69%  624K 4s\n",
            "  7500K .......... .......... .......... .......... .......... 69%  298M 4s\n",
            "  7550K .......... .......... .......... .......... .......... 69%  620K 4s\n",
            "  7600K .......... .......... .......... .......... .......... 70%  627K 4s\n",
            "  7650K .......... .......... .......... .......... .......... 70%  623K 4s\n",
            "  7700K .......... .......... .......... .......... .......... 71%  627K 4s\n",
            "  7750K .......... .......... .......... .......... .......... 71%  122M 4s\n",
            "  7800K .......... .......... .......... .......... .......... 72%  628K 4s\n",
            "  7850K .......... .......... .......... .......... .......... 72%  623K 4s\n",
            "  7900K .......... .......... .......... .......... .......... 73%  628K 4s\n",
            "  7950K .......... .......... .......... .......... .......... 73%  438M 4s\n",
            "  8000K .......... .......... .......... .......... .......... 74%  626K 4s\n",
            "  8050K .......... .......... .......... .......... .......... 74%  624K 4s\n",
            "  8100K .......... .......... .......... .......... .......... 74%  625K 4s\n",
            "  8150K .......... .......... .......... .......... .......... 75%  626K 3s\n",
            "  8200K .......... .......... .......... .......... .......... 75%  122M 3s\n",
            "  8250K .......... .......... .......... .......... .......... 76%  623K 3s\n",
            "  8300K .......... .......... .......... .......... .......... 76%  628K 3s\n",
            "  8350K .......... .......... .......... .......... .......... 77%  624K 3s\n",
            "  8400K .......... .......... .......... .......... .......... 77%  624K 3s\n",
            "  8450K .......... .......... .......... .......... .......... 78%  135M 3s\n",
            "  8500K .......... .......... .......... .......... .......... 78%  624K 3s\n",
            "  8550K .......... .......... .......... .......... .......... 79%  625K 3s\n",
            "  8600K .......... .......... .......... .......... .......... 79%  623K 3s\n",
            "  8650K .......... .......... .......... .......... .......... 80%  165M 3s\n",
            "  8700K .......... .......... .......... .......... .......... 80%  625K 3s\n",
            "  8750K .......... .......... .......... .......... .......... 80%  625K 3s\n",
            "  8800K .......... .......... .......... .......... .......... 81%  623K 3s\n",
            "  8850K .......... .......... .......... .......... .......... 81%  626K 3s\n",
            "  8900K .......... .......... .......... .......... .......... 82% 78.9M 2s\n",
            "  8950K .......... .......... .......... .......... .......... 82%  626K 2s\n",
            "  9000K .......... .......... .......... .......... .......... 83%  622K 2s\n",
            "  9050K .......... .......... .......... .......... .......... 83%  626K 2s\n",
            "  9100K .......... .......... .......... .......... .......... 84%  309M 2s\n",
            "  9150K .......... .......... .......... .......... .......... 84%  624K 2s\n",
            "  9200K .......... .......... .......... .......... .......... 85%  623K 2s\n",
            "  9250K .......... .......... .......... .......... .......... 85%  623K 2s\n",
            "  9300K .......... .......... .......... .......... .......... 86%  629K 2s\n",
            "  9350K .......... .......... .......... .......... .......... 86%  105M 2s\n",
            "  9400K .......... .......... .......... .......... .......... 86%  621K 2s\n",
            "  9450K .......... .......... .......... .......... .......... 87%  622K 2s\n",
            "  9500K .......... .......... .......... .......... .......... 87%  625K 2s\n",
            "  9550K .......... .......... .......... .......... .......... 88%  124M 2s\n",
            "  9600K .......... .......... .......... .......... .......... 88%  624K 2s\n",
            "  9650K .......... .......... .......... .......... .......... 89%  624K 2s\n",
            "  9700K .......... .......... .......... .......... .......... 89%  624K 1s\n",
            "  9750K .......... .......... .......... .......... .......... 90%  621K 1s\n",
            "  9800K .......... .......... .......... .......... .......... 90% 60.4M 1s\n",
            "  9850K .......... .......... .......... .......... .......... 91%  633K 1s\n",
            "  9900K .......... .......... .......... .......... .......... 91%  625K 1s\n",
            "  9950K .......... .......... .......... .......... .......... 92%  623K 1s\n",
            " 10000K .......... .......... .......... .......... .......... 92%  627K 1s\n",
            " 10050K .......... .......... .......... .......... .......... 92%  283M 1s\n",
            " 10100K .......... .......... .......... .......... .......... 93%  626K 1s\n",
            " 10150K .......... .......... .......... .......... .......... 93%  623K 1s\n",
            " 10200K .......... .......... .......... .......... .......... 94%  624K 1s\n",
            " 10250K .......... .......... .......... .......... .......... 94%  420M 1s\n",
            " 10300K .......... .......... .......... .......... .......... 95%  625K 1s\n",
            " 10350K .......... .......... .......... .......... .......... 95%  628K 1s\n",
            " 10400K .......... .......... .......... .......... .......... 96%  627K 1s\n",
            " 10450K .......... .......... .......... .......... .......... 96%  624K 0s\n",
            " 10500K .......... .......... .......... .......... .......... 97%  142M 0s\n",
            " 10550K .......... .......... .......... .......... .......... 97%  625K 0s\n",
            " 10600K .......... .......... .......... .......... .......... 98%  627K 0s\n",
            " 10650K .......... .......... .......... .......... .......... 98%  624K 0s\n",
            " 10700K .......... .......... .......... .......... .......... 98%  190M 0s\n",
            " 10750K .......... .......... .......... .......... .......... 99%  625K 0s\n",
            " 10800K .......... .......... .......... .......... .......... 99%  624K 0s\n",
            " 10850K .......... .......                                    100%  356M=14s\n",
            "\n",
            "2022-02-09 15:10:28 (775 KB/s) - ‘Roms.rar’ saved [11128004/11128004]\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/atari_py/import_roms.py\", line 93, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/atari_py/import_roms.py\", line 89, in main\n",
            "    import_roms(args.dirpath)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/atari_py/import_roms.py\", line 74, in import_roms\n",
            "    with open(filepath, \"rb\") as f:\n",
            "OSError: [Errno 95] Operation not supported: './gdrive/MyDrive/Error Propagation.gdraw'\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install gym > /dev/null 2>&1\n",
        "pip install pyglet > /dev/null 2>&1\n",
        "pip install atari-py > /dev/null 2>&1\n",
        "\n",
        "sudo apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "pip install -U colabgymrender > /dev/null 2>&1\n",
        "\n",
        "# Download and install Atari ROMs\n",
        "# https://github.com/openai/atari-py#roms\n",
        "# http://www.atarimania.com/rom_collection_archive_atari_2600_roms.html\n",
        "wget http://www.atarimania.com/roms/Roms.rar\n",
        "unrar e Roms.rar\n",
        "python -m atari_py.import_roms .\n",
        "\n",
        "export DISPLAY=localhost:0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import random\n",
        "import time\n",
        "from typing import Any, List, Sequence, Tuple\n",
        "from os.path import exists\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import cv2\n",
        "\n",
        "from tensorflow.keras.models import Sequential, clone_model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "\n",
        "from colabgymrender.recorder import Recorder\n",
        "\n",
        "import statistics\n",
        "import tqdm\n",
        "\n",
        "from gym.wrappers import AtariPreprocessing, FrameStack\n",
        "from gym.spaces import Discrete\n"
      ],
      "metadata": {
        "id": "XbGcOsPZHcNv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd5f67c-feab-4c18-b0f7-8468d26efa32"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b3334144/45929032 bytes (7.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b6569984/45929032 bytes (14.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b9879552/45929032 bytes (21.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13197312/45929032 bytes (28.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16400384/45929032 bytes (35.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19873792/45929032 bytes (43.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b23093248/45929032 bytes (50.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26460160/45929032 bytes (57.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29622272/45929032 bytes (64.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32841728/45929032 bytes (71.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36208640/45929032 bytes (78.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b39559168/45929032 bytes (86.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b42860544/45929032 bytes (93.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RescaleActionDiscrete(gym.ActionWrapper):\n",
        "\n",
        "    def __init__(self, env):\n",
        "\n",
        "        super().__init__(env)\n",
        "\n",
        "        self.action_space = Discrete(2)\n",
        "\n",
        "    def action(self, action):\n",
        "        \n",
        "        return action + 2\n",
        "\n",
        "class ReshapeWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        self.shape = shape\n",
        "    \n",
        "    def observation(self, obs):\n",
        "        obs = tf.reshape(obs, self.shape)\n",
        "        return obs"
      ],
      "metadata": {
        "id": "nj--JqcYQMGE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tT4N3qYviUJr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create the environment\n",
        "env = gym.make(\"PongNoFrameskip-v4\")\n",
        "\n",
        "env = AtariPreprocessing(env, scale_obs=True)\n",
        "env = FrameStack(env, 4)\n",
        "env = ReshapeWrapper(env, (1, 84, 84, 4))\n",
        "env = RescaleActionDiscrete(env) # Limit actions to just up and down\n",
        "# env = Recorder(env, 'recordings')\n",
        "\n",
        "# Set seed for experiment reproducibility\n",
        "seed = 42\n",
        "env.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Small epsilon value for stabilizing division operations\n",
        "eps = np.finfo(np.float32).eps.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOUCe2D0iUJu"
      },
      "source": [
        "## Model\n",
        "\n",
        "The *Actor* and *Critic* will be modeled using one neural network that generates the action probabilities and critic value respectively. This tutorial uses model subclassing to define the model. \n",
        "\n",
        "During the forward pass, the model will take in the state as the input and will output both action probabilities and critic value $V$, which models the state-dependent [value function](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#value-functions). The goal is to train a model that chooses actions based on a policy $\\pi$ that maximizes expected [return](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#reward-and-return).\n",
        "\n",
        "For Cartpole-v0, there are four values representing the state: cart position, cart-velocity, pole angle and pole velocity respectively. The agent can take two actions to push the cart left (0) and right (1) respectively.\n",
        "\n",
        "Refer to [OpenAI Gym's CartPole-v0 wiki page](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf) for more information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aXKbbMC-kmuv"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(tf.keras.Model):\n",
        "  \"\"\"Combined actor-critic network.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self, \n",
        "      num_actions: int, \n",
        "      num_hidden_units: int):\n",
        "    \"\"\"Initialize.\"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # self.input_layer = Input((84,84,4))\n",
        "    input_shape = (1, 84, 84, 4)\n",
        "    self.conv0 = Conv2D(filters = 32,kernel_size = (8,8),strides = 4,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2), input_shape=input_shape[1:])\n",
        "    self.conv1 = Conv2D(filters = 64,kernel_size = (4,4),strides = 2,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2))\n",
        "    self.conv2 = Conv2D(filters = 64,kernel_size = (3,3),strides = 1,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2))\n",
        "    self.flatten = Flatten()\n",
        "    # model.add(Dense(512,activation = 'relu', kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
        "    # model.add(Dense(len(self.possible_actions), activation = 'linear')\n",
        "\n",
        "    self.common = Dense(num_hidden_units, activation=\"relu\")\n",
        "    self.actor = Dense(num_actions)\n",
        "    self.critic = Dense(1)\n",
        "\n",
        "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    # x = self.input_layer(inputs)\n",
        "    # inputs = tf.reshape(inputs, (1, 84, 84, 4))\n",
        "    x = self.conv0(inputs)\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.common(x)\n",
        "    return self.actor(x), self.critic(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nWyxJgjLn68c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1647eab-7325-4dc6-87cf-5cb6adcab3f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "num_actions = env.action_space.n  # 2\n",
        "print(env.action_space.n)\n",
        "num_hidden_units = 128\n",
        "\n",
        "model = ActorCritic(num_actions, num_hidden_units)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.state"
      ],
      "metadata": {
        "id": "PwZnlP1eYXcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk92njFziUJw"
      },
      "source": [
        "## Training\n",
        "\n",
        "To train the agent, you will follow these steps:\n",
        "\n",
        "1. Run the agent on the environment to collect training data per episode.\n",
        "2. Compute expected return at each time step.\n",
        "3. Compute the loss for the combined actor-critic model.\n",
        "4. Compute gradients and update network parameters.\n",
        "5. Repeat 1-4 until either success criterion or max episodes has been reached.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2nde2XDs8Gh"
      },
      "source": [
        "### 1. Collecting training data\n",
        "\n",
        "As in supervised learning, in order to train the actor-critic model, you need\n",
        "to have training data. However, in order to collect such data, the model would\n",
        "need to be \"run\" in the environment.\n",
        "\n",
        "Training data is collected for each episode. Then at each time step, the model's forward pass will be run on the environment's state in order to generate action probabilities and the critic value based on the current policy parameterized by the model's weights.\n",
        "\n",
        "The next action will be sampled from the action probabilities generated by the model, which would then be applied to the environment, causing the next state and reward to be generated.\n",
        "\n",
        "This process is implemented in the `run_episode` function, which uses TensorFlow operations so that it can later be compiled into a TensorFlow graph for faster training. Note that `tf.TensorArray`s were used to support Tensor iteration on variable length arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5URrbGlDSAGx"
      },
      "outputs": [],
      "source": [
        "# Wrap OpenAI Gym's `env.step` call as an operation in a TensorFlow function.\n",
        "# This would allow it to be included in a callable TensorFlow graph.\n",
        "\n",
        "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
        "\n",
        "  state, reward, done, _ = env.step(action)\n",
        "    \n",
        "#   return (state.astype(np.float32), \n",
        "  return (state.__array__(np.float32), \n",
        "          np.array(reward, np.int32), \n",
        "          np.array(done, np.int32))\n",
        "\n",
        "\n",
        "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
        "  return tf.numpy_function(env_step, [action], \n",
        "                           [tf.float32, tf.int32, tf.int32])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "a4qVRV063Cl9"
      },
      "outputs": [],
      "source": [
        "def run_episode(\n",
        "    initial_state: tf.Tensor,  \n",
        "    model: tf.keras.Model, \n",
        "    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Runs a single episode to collect training data.\"\"\"\n",
        "\n",
        "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
        "\n",
        "  initial_state_shape = initial_state.shape\n",
        "  state = initial_state\n",
        "\n",
        "  for t in tf.range(max_steps):\n",
        "    # Convert state into a batched tensor (batch size = 1)\n",
        "    state = tf.expand_dims(state, 0)\n",
        "  \n",
        "    # Run the model and to get action probabilities and critic value\n",
        "    action_logits_t, value = model(state)\n",
        "  \n",
        "    # Sample next action from the action probability distribution\n",
        "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
        "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
        "\n",
        "    # Store critic values\n",
        "    values = values.write(t, tf.squeeze(value))\n",
        "\n",
        "    # Store log probability of the action chosen\n",
        "    action_probs = action_probs.write(t, action_probs_t[0, action])\n",
        "  \n",
        "    # Apply action to the environment to get next state and reward\n",
        "    state, reward, done = tf_env_step(action)\n",
        "    state.set_shape(initial_state_shape)\n",
        "  \n",
        "    # Store reward\n",
        "    rewards = rewards.write(t, reward)\n",
        "\n",
        "    if tf.cast(done, tf.bool):\n",
        "      break\n",
        "\n",
        "  action_probs = action_probs.stack()\n",
        "  values = values.stack()\n",
        "  rewards = rewards.stack()\n",
        "  \n",
        "  return action_probs, values, rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBnIHdz22dIx"
      },
      "source": [
        "### 2. Computing expected returns\n",
        "\n",
        "The sequence of rewards for each timestep $t$, $\\{r_{t}\\}^{T}_{t=1}$ collected during one episode is converted into a sequence of expected returns $\\{G_{t}\\}^{T}_{t=1}$ in which the sum of rewards is taken from the current timestep $t$ to $T$ and each reward is multiplied with an exponentially decaying discount factor $\\gamma$:\n",
        "\n",
        "$$G_{t} = \\sum^{T}_{t'=t} \\gamma^{t'-t}r_{t'}$$\n",
        "\n",
        "Since $\\gamma\\in(0,1)$, rewards further out from the current timestep are given less weight.\n",
        "\n",
        "Intuitively, expected return simply implies that rewards now are better than rewards later. In a mathematical sense, it is to ensure that the sum of the rewards converges.\n",
        "\n",
        "To stabilize training, the resulting sequence of returns is also standardized (i.e. to have zero mean and unit standard deviation).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jpEwFyl315dl"
      },
      "outputs": [],
      "source": [
        "def get_expected_return(\n",
        "    rewards: tf.Tensor, \n",
        "    gamma: float, \n",
        "    standardize: bool = True) -> tf.Tensor:\n",
        "  \"\"\"Compute expected returns per timestep.\"\"\"\n",
        "\n",
        "  n = tf.shape(rewards)[0]\n",
        "  returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
        "\n",
        "  # Start from the end of `rewards` and accumulate reward sums\n",
        "  # into the `returns` array\n",
        "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
        "  discounted_sum = tf.constant(0.0)\n",
        "  discounted_sum_shape = discounted_sum.shape\n",
        "  for i in tf.range(n):\n",
        "    reward = rewards[i]\n",
        "    discounted_sum = reward + gamma * discounted_sum\n",
        "    discounted_sum.set_shape(discounted_sum_shape)\n",
        "    returns = returns.write(i, discounted_sum)\n",
        "  returns = returns.stack()[::-1]\n",
        "\n",
        "  if standardize:\n",
        "    returns = ((returns - tf.math.reduce_mean(returns)) / \n",
        "               (tf.math.reduce_std(returns) + eps))\n",
        "\n",
        "  return returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hrPLrgGxlvb"
      },
      "source": [
        "### 3. The actor-critic loss\n",
        "\n",
        "Since a hybrid actor-critic model is used, the chosen loss function is a combination of actor and critic losses for training, as shown below:\n",
        "\n",
        "$$L = L_{actor} + L_{critic}$$\n",
        "\n",
        "#### Actor loss\n",
        "\n",
        "The actor loss is based on [policy gradients with the critic as a state dependent baseline](https://www.youtube.com/watch?v=EKqxumCuAAY&t=62m23s) and computed with single-sample (per-episode) estimates.\n",
        "\n",
        "$$L_{actor} = -\\sum^{T}_{t=1} \\log\\pi_{\\theta}(a_{t} | s_{t})[G(s_{t}, a_{t})  - V^{\\pi}_{\\theta}(s_{t})]$$\n",
        "\n",
        "where:\n",
        "- $T$: the number of timesteps per episode, which can vary per episode\n",
        "- $s_{t}$: the state at timestep $t$\n",
        "- $a_{t}$: chosen action at timestep $t$ given state $s$\n",
        "- $\\pi_{\\theta}$: is the policy (actor) parameterized by $\\theta$\n",
        "- $V^{\\pi}_{\\theta}$: is the value function (critic) also parameterized by $\\theta$\n",
        "- $G = G_{t}$: the expected return for a given state, action pair at timestep $t$\n",
        "\n",
        "A negative term is added to the sum since the idea is to maximize the probabilities of actions yielding higher rewards by minimizing the combined loss.\n",
        "\n",
        "<br>\n",
        "\n",
        "##### Advantage\n",
        "\n",
        "The $G - V$ term in our $L_{actor}$ formulation is called the [advantage](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#advantage-functions), which indicates how much better an action is given a particular state over a random action selected according to the policy $\\pi$ for that state.\n",
        "\n",
        "While it's possible to exclude a baseline, this may result in high variance during training. And the nice thing about choosing the critic $V$ as a baseline is that it trained to be as close as possible to $G$, leading to a lower variance.\n",
        "\n",
        "In addition, without the critic, the algorithm would try to increase probabilities for actions taken on a particular state based on expected return, which may not make much of a difference if the relative probabilities between actions remain the same.\n",
        "\n",
        "For instance, suppose that two actions for a given state would yield the same expected return. Without the critic, the algorithm would try to raise the probability of these actions based on the objective $J$. With the critic, it may turn out that there's no advantage ($G - V = 0$) and thus no benefit gained in increasing the actions' probabilities and the algorithm would set the gradients to zero.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Critic loss\n",
        "\n",
        "Training $V$ to be as close possible to $G$ can be set up as a regression problem with the following loss function:\n",
        "\n",
        "$$L_{critic} = L_{\\delta}(G, V^{\\pi}_{\\theta})$$\n",
        "\n",
        "where $L_{\\delta}$ is the [Huber loss](https://en.wikipedia.org/wiki/Huber_loss), which is less sensitive to outliers in data than squared-error loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9EXwbEez6n9m"
      },
      "outputs": [],
      "source": [
        "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "def compute_loss(\n",
        "    action_probs: tf.Tensor,  \n",
        "    values: tf.Tensor,  \n",
        "    returns: tf.Tensor) -> tf.Tensor:\n",
        "  \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
        "\n",
        "  advantage = returns - values\n",
        "\n",
        "  action_log_probs = tf.math.log(action_probs)\n",
        "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
        "\n",
        "  critic_loss = huber_loss(values, returns)\n",
        "\n",
        "  return actor_loss + critic_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSYkQOmRfV75"
      },
      "source": [
        "### 4. Defining the training step to update parameters\n",
        "\n",
        "All of the steps above are combined into a training step that is run every episode. All steps leading up to the loss function are executed with the `tf.GradientTape` context to enable automatic differentiation.\n",
        "\n",
        "This tutorial uses the Adam optimizer to apply the gradients to the model parameters.\n",
        "\n",
        "The sum of the undiscounted rewards, `episode_reward`, is also computed in this step. This value will be used later on to evaluate if the success criterion is met.\n",
        "\n",
        "The `tf.function` context is applied to the `train_step` function so that it can be compiled into a callable TensorFlow graph, which can lead to 10x speedup in training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QoccrkF3IFCg"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(\n",
        "    initial_state: tf.Tensor, \n",
        "    model: tf.keras.Model, \n",
        "    optimizer: tf.keras.optimizers.Optimizer, \n",
        "    gamma: float, \n",
        "    max_steps_per_episode: int) -> tf.Tensor:\n",
        "  \"\"\"Runs a model training step.\"\"\"\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # Run the model for one episode to collect training data\n",
        "    action_probs, values, rewards = run_episode(\n",
        "        initial_state, model, max_steps_per_episode) \n",
        "\n",
        "    # Calculate expected returns\n",
        "    returns = get_expected_return(rewards, gamma)\n",
        "\n",
        "    # Convert training data to appropriate TF tensor shapes\n",
        "    action_probs, values, returns = [\n",
        "        tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
        "\n",
        "    # Calculating loss values to update our network\n",
        "    loss = compute_loss(action_probs, values, returns)\n",
        "\n",
        "  # Compute the gradients from the loss\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "  # Apply the gradients to the model's parameters\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  episode_reward = tf.math.reduce_sum(rewards)\n",
        "\n",
        "  return episode_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFvZiDoAflGK"
      },
      "source": [
        "### 5. Run the training loop\n",
        "\n",
        "Training is executed by running the training step until either the success criterion or maximum number of episodes is reached.  \n",
        "\n",
        "A running record of episode rewards is kept in a queue. Once 100 trials are reached, the oldest reward is removed at the left (tail) end of the queue and the newest one is added at the head (right). A running sum of the rewards is also maintained for computational efficiency. \n",
        "\n",
        "Depending on your runtime, training can finish in less than a minute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kbmBxnzLiUJx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "a807f9ca-2f89-43fe-99b6-7f1cfffa0946"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-5e0c221b6c77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nmin_episodes_criterion = 100\\nmax_episodes = 10000\\nmax_steps_per_episode = 1000\\n\\n# Cartpole-v0 is considered solved if average reward is >= 195 over 100 \\n# consecutive trials\\nreward_threshold = 20\\nrunning_reward = 0\\n\\n# Discount factor for future rewards\\ngamma = 0.99\\n\\n# Keep last episodes reward\\nepisodes_reward: deque = deque(maxlen=min_episodes_criterion)\\n\\nweights_path = \\'/content/gdrive/My Drive/Colab Notebooks/pong/recent_weights.hdf5\\'\\n\\nif exists(weights_path):\\n    model.load_weights(weights_path)\\n    print(\"weights loaded from {}\".format(weights_path))\\n\\nwith tqdm.trange(max_episodes) as t:\\n  for i in t:\\n    initial_state = tf.constant(env.reset(), dtype=tf.float32)\\n    episode_reward = int(train_step(\\n        initial_state, model, optimizer, gamma, max_steps_per_episode))\\n    \\n    episodes_reward.append(episode_reward)\\n    running_reward = statistics.mean(episodes_reward)\\n  \\n    t.set_description(f\\'Episode {i}\\')\\n    t.set_postfix(\\n        episode_reward=episode_reward, running_reward=running_reward)\\n  \\n    # Show average episode reward every 10 episodes\\n    if i % 10 == 0:\\n      pass # print(f\\'Episode {i}: average reward: {avg_reward}\\')\\n\\n    # Save model every 100 episodes\\n    if i % 100 == 0:\\n        model.save_weights(weights_path)\\n        print(\"\\\\...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2399\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2400\u001b[0m         raise ValueError(\n\u001b[0;32m-> 2401\u001b[0;31m             \u001b[0;34m'Unable to load weights saved in HDF5 format into a subclassed '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2402\u001b[0m             \u001b[0;34m'Model which has not created its variables yet. Call the Model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2403\u001b[0m             'first, then load the weights.')\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights."
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "min_episodes_criterion = 100\n",
        "max_episodes = 10000\n",
        "max_steps_per_episode = 1000\n",
        "\n",
        "# Cartpole-v0 is considered solved if average reward is >= 195 over 100 \n",
        "# consecutive trials\n",
        "reward_threshold = 20\n",
        "running_reward = 0\n",
        "\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "\n",
        "# Keep last episodes reward\n",
        "episodes_reward: deque = deque(maxlen=min_episodes_criterion)\n",
        "\n",
        "weights_path = '/content/gdrive/My Drive/Colab Notebooks/pong/recent_weights.hdf5'\n",
        "\n",
        "if exists(weights_path):\n",
        "    model.load_weights(weights_path)\n",
        "    print(\"weights loaded from {}\".format(weights_path))\n",
        "\n",
        "with tqdm.trange(max_episodes) as t:\n",
        "  for i in t:\n",
        "    initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "    episode_reward = int(train_step(\n",
        "        initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
        "    \n",
        "    episodes_reward.append(episode_reward)\n",
        "    running_reward = statistics.mean(episodes_reward)\n",
        "  \n",
        "    t.set_description(f'Episode {i}')\n",
        "    t.set_postfix(\n",
        "        episode_reward=episode_reward, running_reward=running_reward)\n",
        "  \n",
        "    # Show average episode reward every 10 episodes\n",
        "    if i % 10 == 0:\n",
        "      pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
        "\n",
        "    # Save model every 100 episodes\n",
        "    if i % 100 == 0:\n",
        "        model.save_weights(weights_path)\n",
        "        print(\"\\nsaved weights to {}\".format(weights_path))\n",
        "  \n",
        "    if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
        "        break\n",
        "\n",
        "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru8BEwS1EmAv"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "After training, it would be good to visualize how the model performs in the environment. You can run the cells below to generate a GIF animation of one episode run of the model. Note that additional packages need to be installed for OpenAI Gym to render the environment's images correctly in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbIMMkfmRHyC"
      },
      "outputs": [],
      "source": [
        "# Render an episode and save as a GIF file\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "\n",
        "def render_episode(env: gym.Env, model: tf.keras.Model, max_steps: int): \n",
        "  screen = env.render(mode='rgb_array')\n",
        "  im = Image.fromarray(screen)\n",
        "\n",
        "  images = [im]\n",
        "  \n",
        "  state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "  for i in range(1, max_steps + 1):\n",
        "    state = tf.expand_dims(state, 0)\n",
        "    action_probs, _ = model(state)\n",
        "    action = np.argmax(np.squeeze(action_probs))\n",
        "\n",
        "    state, _, done, _ = env.step(action)\n",
        "    state = tf.constant(state, dtype=tf.float32)\n",
        "\n",
        "    # Render screen every 10 steps\n",
        "    if i % 10 == 0:\n",
        "      screen = env.render(mode='rgb_array')\n",
        "      images.append(Image.fromarray(screen))\n",
        "  \n",
        "    if done:\n",
        "      break\n",
        "  \n",
        "  return images\n",
        "\n",
        "\n",
        "# Save GIF image\n",
        "images = render_episode(env, model, max_steps_per_episode)\n",
        "image_file = 'cartpole-v0.gif'\n",
        "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
        "images[0].save(\n",
        "    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLd720SejKmf"
      },
      "outputs": [],
      "source": [
        "import tensorflow_docs.vis.embed as embed\n",
        "embed.embed_file(image_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnq9Hzo1Po6X"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "This tutorial demonstrated how to implement the actor-critic method using Tensorflow.\n",
        "\n",
        "As a next step, you could try training a model on a different environment in OpenAI Gym. \n",
        "\n",
        "For additional information regarding actor-critic methods and the Cartpole-v0 problem, you may refer to the following resources:\n",
        "\n",
        "- [Actor Critic Method](https://hal.inria.fr/hal-00840470/document)\n",
        "- [Actor Critic Lecture (CAL)](https://www.youtube.com/watch?v=EKqxumCuAAY&list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&index=7&t=0s)\n",
        "- [Cartpole learning control problem \\[Barto, et al. 1983\\]](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf) \n",
        "\n",
        "For more reinforcement learning examples in TensorFlow, you can check the following resources:\n",
        "- [Reinforcement learning code examples (keras.io)](https://keras.io/examples/rl/)\n",
        "- [TF-Agents reinforcement learning library](https://www.tensorflow.org/agents)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_jQ1tEQCxwRx"
      ],
      "name": "actor_critic.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}