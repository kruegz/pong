{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pong.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_jQ1tEQCxwRx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kruegz/pong/blob/main/pong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p62G8M_viUJp"
      },
      "source": [
        "# Playing Pong with the Actor-Critic Method\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://gym.openai.com/envs/Pong-v0/\n",
        "https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af\n",
        "https://towardsdatascience.com/getting-an-ai-to-play-atari-pong-with-deep-reinforcement-learning-47b0c56e78ae\n"
      ],
      "metadata": {
        "id": "QbCoedrbk1Oj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glLwIctHiUJq"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Import necessary packages and configure global settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBeQhPi2S4m5"
      },
      "source": [
        "%%bash\n",
        "\n",
        "# Install main packages\n",
        "pip install gym > /dev/null 2>&1\n",
        "pip install pyglet > /dev/null 2>&1\n",
        "pip install atari-py > /dev/null 2>&1\n",
        "\n",
        "# Install additional packages for visualization\n",
        "sudo apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1\n",
        "\n",
        "\n",
        "# Download and install Atari ROMs\n",
        "# https://github.com/openai/atari-py#roms\n",
        "# http://www.atarimania.com/rom_collection_archive_atari_2600_roms.html\n",
        "wget http://www.atarimania.com/roms/Roms.rar\n",
        "unrar e Roms.rar\n",
        "python -m atari_py.import_roms .\n",
        "\n",
        "export DISPLAY=localhost:0.0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import random\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import cv2\n",
        "\n",
        "from tensorflow.keras.models import Sequential, clone_model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras.backend as K"
      ],
      "metadata": {
        "id": "JH9AxXJuc1j_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Memory():\n",
        "    def __init__(self,max_len):\n",
        "        self.max_len = max_len\n",
        "        self.frames = deque(maxlen = max_len)\n",
        "        self.actions = deque(maxlen = max_len)\n",
        "        self.rewards = deque(maxlen = max_len)\n",
        "        self.done_flags = deque(maxlen = max_len)\n",
        "\n",
        "    def add_experience(self,next_frame, next_frames_reward, next_action, next_frame_terminal):\n",
        "        self.frames.append(next_frame)\n",
        "        self.actions.append(next_action)\n",
        "        self.rewards.append(next_frames_reward)\n",
        "        self.done_flags.append(next_frame_terminal)"
      ],
      "metadata": {
        "id": "WPtDtkmAdh1u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_frame(frame):\n",
        "    frame = frame[30:-12,5:-4]\n",
        "    frame = np.average(frame,axis = 2)\n",
        "    frame = cv2.resize(frame,(84,84),interpolation = cv2.INTER_NEAREST)\n",
        "    frame = np.array(frame,dtype = np.uint8)\n",
        "    return frame\n",
        "\n",
        "def initialize_new_game(name, env, agent):\n",
        "    \"\"\"We don't want an agents past game influencing its new game, so we add in some dummy data to initialize\"\"\"\n",
        "    \n",
        "    env.reset()\n",
        "    starting_frame = resize_frame(env.step(0)[0])\n",
        "\n",
        "    dummy_action = 0\n",
        "    dummy_reward = 0\n",
        "    dummy_done = False\n",
        "    for i in range(3):\n",
        "        agent.memory.add_experience(starting_frame, dummy_reward, dummy_action, dummy_done)\n",
        "\n",
        "def make_env(name, agent):\n",
        "    env = gym.make(name)\n",
        "    return env\n",
        "\n",
        "def take_step(name, env, agent, score, debug):\n",
        "    \n",
        "    #1 and 2: Update timesteps and save weights\n",
        "    agent.total_timesteps += 1\n",
        "    if agent.total_timesteps % 50000 == 0:\n",
        "      agent.model.save_weights('recent_weights.hdf5')\n",
        "      print('\\nWeights saved!')\n",
        "\n",
        "    #3: Take action\n",
        "    next_frame, next_frames_reward, next_frame_terminal, info = env.step(agent.memory.actions[-1])\n",
        "    \n",
        "    #4: Get next state\n",
        "    next_frame = resize_frame(next_frame)\n",
        "    new_state = [agent.memory.frames[-3], agent.memory.frames[-2], agent.memory.frames[-1], next_frame]\n",
        "    new_state = np.moveaxis(new_state,0,2)/255 #We have to do this to get it into keras's goofy format of [batch_size,rows,columns,channels]\n",
        "    new_state = np.expand_dims(new_state,0) #^^^\n",
        "    \n",
        "    #5: Get next action, using next state\n",
        "    next_action = agent.get_action(new_state)\n",
        "\n",
        "    #6: If game is over, return the score\n",
        "    if next_frame_terminal:\n",
        "        agent.memory.add_experience(next_frame, next_frames_reward, next_action, next_frame_terminal)\n",
        "        return (score + next_frames_reward),True\n",
        "\n",
        "    #7: Now we add the next experience to memory\n",
        "    agent.memory.add_experience(next_frame, next_frames_reward, next_action, next_frame_terminal)\n",
        "\n",
        "    #8: If we are trying to debug this then render\n",
        "    if debug:\n",
        "        env.render()\n",
        "\n",
        "    #9: If the threshold memory is satisfied, make the agent learn from memory\n",
        "    if len(agent.memory.frames) > agent.starting_mem_len:\n",
        "        agent.learn(debug)\n",
        "\n",
        "    return (score + next_frames_reward),False\n",
        "\n",
        "def tf_take_step(name,env,agent,score, debug):\n",
        "    return tf.numpy_function(take_step, [name,env,agent,score, debug], \n",
        "                           [tf.float32, tf.int32, tf.int32])\n",
        "\n",
        "# @tf.function\n",
        "def play_episode(name, env, agent, debug = False):\n",
        "    initialize_new_game(name, env, agent)\n",
        "    done = False\n",
        "    score = 0\n",
        "    while True:\n",
        "        score,done = take_step(name,env,agent,score, debug)\n",
        "        if done:\n",
        "            break\n",
        "    return score\n"
      ],
      "metadata": {
        "id": "TJngELbRdtxO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "    def __init__(self,possible_actions,starting_mem_len,max_mem_len,starting_epsilon,learn_rate, starting_lives = 5, debug = False):\n",
        "        self.memory = Memory(max_mem_len)\n",
        "        self.possible_actions = possible_actions\n",
        "        self.epsilon = starting_epsilon\n",
        "        self.epsilon_decay = .9/100000\n",
        "        self.epsilon_min = .05\n",
        "        self.gamma = .95\n",
        "        self.learn_rate = learn_rate\n",
        "        self.model = self._build_model()\n",
        "        self.model_target = clone_model(self.model)\n",
        "        self.total_timesteps = 0\n",
        "        self.lives = starting_lives #this parameter does not apply to pong\n",
        "        self.starting_mem_len = starting_mem_len\n",
        "        self.learns = 0\n",
        "\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Input((84,84,4)))\n",
        "        model.add(Conv2D(filters = 32,kernel_size = (8,8),strides = 4,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
        "        model.add(Conv2D(filters = 64,kernel_size = (4,4),strides = 2,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
        "        model.add(Conv2D(filters = 64,kernel_size = (3,3),strides = 1,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512,activation = 'relu', kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
        "        model.add(Dense(len(self.possible_actions), activation = 'linear'))\n",
        "        optimizer = Adam(self.learn_rate)\n",
        "        model.compile(optimizer, loss=tf.keras.losses.Huber())\n",
        "        model.summary()\n",
        "        print('\\nAgent Initialized\\n')\n",
        "        return model\n",
        "\n",
        "    def get_action(self,state):\n",
        "        \"\"\"Explore\"\"\"\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return random.sample(self.possible_actions,1)[0]\n",
        "\n",
        "        \"\"\"Do Best Acton\"\"\"\n",
        "        a_index = np.argmax(self.model.predict(state))\n",
        "        return self.possible_actions[a_index]\n",
        "\n",
        "    def _index_valid(self,index):\n",
        "        if self.memory.done_flags[index-3] or self.memory.done_flags[index-2] or self.memory.done_flags[index-1] or self.memory.done_flags[index]:\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "\n",
        "    def learn(self,debug = False):\n",
        "        \"\"\"we want the output[a] to be R_(t+1) + Qmax_(t+1).\"\"\"\n",
        "        \"\"\"So target for taking action 1 should be [output[0], R_(t+1) + Qmax_(t+1), output[2]]\"\"\"\n",
        "\n",
        "        \"\"\"First we need 32 random valid indicies\"\"\"\n",
        "        states = []\n",
        "        next_states = []\n",
        "        actions_taken = []\n",
        "        next_rewards = []\n",
        "        next_done_flags = []\n",
        "\n",
        "        while len(states) < 32:\n",
        "            index = np.random.randint(4,len(self.memory.frames) - 1)\n",
        "            if self._index_valid(index):\n",
        "                state = [self.memory.frames[index-3], self.memory.frames[index-2], self.memory.frames[index-1], self.memory.frames[index]]\n",
        "                state = np.moveaxis(state,0,2)/255\n",
        "                next_state = [self.memory.frames[index-2], self.memory.frames[index-1], self.memory.frames[index], self.memory.frames[index+1]]\n",
        "                next_state = np.moveaxis(next_state,0,2)/255\n",
        "\n",
        "                states.append(state)\n",
        "                next_states.append(next_state)\n",
        "                actions_taken.append(self.memory.actions[index])\n",
        "                next_rewards.append(self.memory.rewards[index+1])\n",
        "                next_done_flags.append(self.memory.done_flags[index+1])\n",
        "\n",
        "        \"\"\"Now we get the ouputs from our model, and the target model. We need this for our target in the error function\"\"\"\n",
        "        labels = self.model.predict(np.array(states))\n",
        "        next_state_values = self.model_target.predict(np.array(next_states))\n",
        "        \n",
        "        \"\"\"Now we define our labels, or what the output should have been\n",
        "           We want the output[action_taken] to be R_(t+1) + Qmax_(t+1) \"\"\"\n",
        "        for i in range(32):\n",
        "            action = self.possible_actions.index(actions_taken[i])\n",
        "            labels[i][action] = next_rewards[i] + (not next_done_flags[i]) * self.gamma * max(next_state_values[i])\n",
        "\n",
        "        \"\"\"Train our model using the states and outputs generated\"\"\"\n",
        "        self.model.fit(np.array(states),labels,batch_size = 32, epochs = 1, verbose = 0)\n",
        "\n",
        "        \"\"\"Decrease epsilon and update how many times our agent has learned\"\"\"\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon -= self.epsilon_decay\n",
        "        self.learns += 1\n",
        "        \n",
        "        \"\"\"Every 10000 learned, copy our model weights to our target model\"\"\"\n",
        "        if self.learns % 10000 == 0:\n",
        "            self.model_target.set_weights(self.model.get_weights())\n",
        "            print('\\nTarget model updated')"
      ],
      "metadata": {
        "id": "RUsLdyCld37G"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'Pong-v0'\n",
        "\n",
        "agent = Agent(possible_actions=[0,2,3],starting_mem_len=50000,max_mem_len=750000,starting_epsilon = 1, learn_rate = .00025)\n",
        "env = make_env(name,agent)\n",
        "\n",
        "last_100_avg = [-21]\n",
        "scores = deque(maxlen = 100)\n",
        "max_score = -21\n",
        "\n",
        "\"\"\" If testing:\n",
        "agent.model.load_weights('recent_weights.hdf5')\n",
        "agent.model_target.load_weights('recent_weights.hdf5')\n",
        "agent.epsilon = 0.0\n",
        "\"\"\"\n",
        "\n",
        "env.reset()\n",
        "\n",
        "for i in range(1000000):\n",
        "    timesteps = agent.total_timesteps\n",
        "    timee = time.time()\n",
        "    score = play_episode(name, env, agent, debug = False) #set debug to true for rendering\n",
        "    scores.append(score)\n",
        "    if score > max_score:\n",
        "        max_score = score\n",
        "\n",
        "    print('\\nEpisode: ' + str(i))\n",
        "    print('Steps: ' + str(agent.total_timesteps - timesteps))\n",
        "    print('Duration: ' + str(time.time() - timee))\n",
        "    print('Score: ' + str(score))\n",
        "    print('Max Score: ' + str(max_score))\n",
        "    print('Epsilon: ' + str(agent.epsilon))\n",
        "\n",
        "    if i%100==0 and i!=0:\n",
        "        last_100_avg.append(sum(scores)/len(scores))\n",
        "        plt.plot(np.arange(0,i+1,100),last_100_avg)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmaDSWhLNS59",
        "outputId": "61ec32cf-7dad-4ada-db16-ce00a5f2938b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 20, 20, 32)        8224      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 9, 9, 64)          32832     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 7, 7, 64)          36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3136)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               1606144   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 1539      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,685,667\n",
            "Trainable params: 1,685,667\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "Agent Initialized\n",
            "\n",
            "\n",
            "Episode: 0\n",
            "Steps: 1416\n",
            "Duration: 1.812547206878662\n",
            "Score: -21.0\n",
            "Max Score: -21\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 1\n",
            "Steps: 1178\n",
            "Duration: 1.495856761932373\n",
            "Score: -21.0\n",
            "Max Score: -21\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 2\n",
            "Steps: 1090\n",
            "Duration: 1.3766977787017822\n",
            "Score: -21.0\n",
            "Max Score: -21\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 3\n",
            "Steps: 1258\n",
            "Duration: 1.5876667499542236\n",
            "Score: -21.0\n",
            "Max Score: -21\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 4\n",
            "Steps: 1216\n",
            "Duration: 1.538200855255127\n",
            "Score: -20.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 5\n",
            "Steps: 1179\n",
            "Duration: 1.4871294498443604\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 6\n",
            "Steps: 1285\n",
            "Duration: 1.6093876361846924\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 7\n",
            "Steps: 1507\n",
            "Duration: 1.9092662334442139\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 8\n",
            "Steps: 1401\n",
            "Duration: 1.7653717994689941\n",
            "Score: -20.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 9\n",
            "Steps: 1344\n",
            "Duration: 1.7048063278198242\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 10\n",
            "Steps: 1609\n",
            "Duration: 2.0260181427001953\n",
            "Score: -20.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 11\n",
            "Steps: 1329\n",
            "Duration: 1.6980817317962646\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 12\n",
            "Steps: 1097\n",
            "Duration: 1.392995834350586\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 13\n",
            "Steps: 1270\n",
            "Duration: 1.6031997203826904\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 14\n",
            "Steps: 1258\n",
            "Duration: 1.5898432731628418\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 15\n",
            "Steps: 1245\n",
            "Duration: 1.5782015323638916\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 16\n",
            "Steps: 1186\n",
            "Duration: 1.5019443035125732\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 17\n",
            "Steps: 1184\n",
            "Duration: 1.487185001373291\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 18\n",
            "Steps: 1187\n",
            "Duration: 1.5030982494354248\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 19\n",
            "Steps: 1250\n",
            "Duration: 1.5852775573730469\n",
            "Score: -19.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 20\n",
            "Steps: 1177\n",
            "Duration: 1.488638162612915\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 21\n",
            "Steps: 1281\n",
            "Duration: 1.6196539402008057\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 22\n",
            "Steps: 1324\n",
            "Duration: 1.6519687175750732\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 23\n",
            "Steps: 1245\n",
            "Duration: 1.5820326805114746\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 24\n",
            "Steps: 1028\n",
            "Duration: 1.2969868183135986\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 25\n",
            "Steps: 1176\n",
            "Duration: 1.4845664501190186\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 26\n",
            "Steps: 1591\n",
            "Duration: 1.996840000152588\n",
            "Score: -19.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 27\n",
            "Steps: 1501\n",
            "Duration: 1.8938491344451904\n",
            "Score: -19.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 28\n",
            "Steps: 1103\n",
            "Duration: 1.393975019454956\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 29\n",
            "Steps: 1228\n",
            "Duration: 1.54365873336792\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 30\n",
            "Steps: 1349\n",
            "Duration: 1.7128229141235352\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 31\n",
            "Steps: 1832\n",
            "Duration: 2.304063320159912\n",
            "Score: -19.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 32\n",
            "Steps: 1181\n",
            "Duration: 1.4888973236083984\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 33\n",
            "Steps: 1265\n",
            "Duration: 1.6019482612609863\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 34\n",
            "Steps: 1629\n",
            "Duration: 2.0565853118896484\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 35\n",
            "Steps: 1508\n",
            "Duration: 1.905857801437378\n",
            "Score: -19.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 36\n",
            "Steps: 1340\n",
            "Duration: 1.6946840286254883\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 37\n",
            "Steps: 1099\n",
            "Duration: 1.3848330974578857\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Weights saved!\n"
          ]
        }
      ]
    }
  ]
}