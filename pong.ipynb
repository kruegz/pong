{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pong.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_jQ1tEQCxwRx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kruegz/pong/blob/main/pong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p62G8M_viUJp"
      },
      "source": [
        "# Playing Pong with the Actor-Critic Method\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mJ2i6jvZ3sK"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/reinforcement_learning/actor_critic.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/reinforcement_learning/actor_critic.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/reinforcement_learning/actor_critic.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFgN7h_wiUJq"
      },
      "source": [
        "This tutorial demonstrates how to implement the [Actor-Critic](https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf) method using TensorFlow to train an agent on the [Open AI Gym](https://gym.openai.com/) Pong is assumed to have some familiarity with [policy gradient methods](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf) of reinforcement learning. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kA10ZKRR0hi"
      },
      "source": [
        "**Actor-Critic methods**\n",
        "\n",
        "Actor-Critic methods are [temporal difference (TD) learning](https://en.wikipedia.org/wiki/Temporal_difference_learning) methods that represent the policy function independent of the value function. \n",
        "\n",
        "A policy function (or policy) returns a probability distribution over actions that the agent can take based on the given state.\n",
        "A value function determines the expected return for an agent starting at a given state and acting according to a particular policy forever after.\n",
        "\n",
        "In the Actor-Critic method, the policy is referred to as the *actor* that proposes a set of possible actions given a state, and the estimated value function is referred to as the *critic*, which evaluates actions taken by the *actor* based on the given policy.\n",
        "\n",
        "In this tutorial, both the *Actor* and *Critic* will be represented using one neural network with two outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://gym.openai.com/envs/Pong-v0/\n",
        "https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af\n",
        "https://towardsdatascience.com/getting-an-ai-to-play-atari-pong-with-deep-reinforcement-learning-47b0c56e78ae\n"
      ],
      "metadata": {
        "id": "QbCoedrbk1Oj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glLwIctHiUJq"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Import necessary packages and configure global settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBeQhPi2S4m5"
      },
      "source": [
        "%%bash\n",
        "\n",
        "# Install main packages\n",
        "pip install gym > /dev/null 2>&1\n",
        "pip install pyglet > /dev/null 2>&1\n",
        "pip install atari-py > /dev/null 2>&1\n",
        "\n",
        "# Install additional packages for visualization\n",
        "sudo apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1\n",
        "\n",
        "\n",
        "# Download and install Atari ROMs\n",
        "# https://github.com/openai/atari-py#roms\n",
        "# http://www.atarimania.com/rom_collection_archive_atari_2600_roms.html\n",
        "wget http://www.atarimania.com/roms/Roms.rar\n",
        "unrar e Roms.rar\n",
        "python -m atari_py.import_roms .\n",
        "\n",
        "export DISPLAY=localhost:0.0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def resize_frame(frame):\n",
        "    frame = frame[30:-12,5:-4]\n",
        "    frame = np.average(frame,axis = 2)\n",
        "    frame = cv2.resize(frame,(84,84),interpolation = cv2.INTER_NEAREST)\n",
        "    frame = np.array(frame,dtype = np.uint8)\n",
        "    return frame\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "class Memory():\n",
        "    def __init__(self,max_len):\n",
        "        self.max_len = max_len\n",
        "        self.frames = deque(maxlen = max_len)\n",
        "        self.actions = deque(maxlen = max_len)\n",
        "        self.rewards = deque(maxlen = max_len)\n",
        "        self.done_flags = deque(maxlen = max_len)\n",
        "\n",
        "    def add_experience(self,next_frame, next_frames_reward, next_action, next_frame_terminal):\n",
        "        self.frames.append(next_frame)\n",
        "        self.actions.append(next_action)\n",
        "        self.rewards.append(next_frames_reward)\n",
        "        self.done_flags.append(next_frame_terminal)\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def initialize_new_game(name, env, agent):\n",
        "    \"\"\"We don't want an agents past game influencing its new game, so we add in some dummy data to initialize\"\"\"\n",
        "    \n",
        "    env.reset()\n",
        "    starting_frame = resize_frame(env.step(0)[0])\n",
        "\n",
        "    dummy_action = 0\n",
        "    dummy_reward = 0\n",
        "    dummy_done = False\n",
        "    for i in range(3):\n",
        "        agent.memory.add_experience(starting_frame, dummy_reward, dummy_action, dummy_done)\n",
        "\n",
        "def make_env(name, agent):\n",
        "    env = gym.make(name)\n",
        "    return env\n",
        "\n",
        "def take_step(name, env, agent, score, debug):\n",
        "    \n",
        "    #1 and 2: Update timesteps and save weights\n",
        "    agent.total_timesteps += 1\n",
        "    if agent.total_timesteps % 50000 == 0:\n",
        "      agent.model.save_weights('recent_weights.hdf5')\n",
        "      print('\\nWeights saved!')\n",
        "\n",
        "    #3: Take action\n",
        "    next_frame, next_frames_reward, next_frame_terminal, info = env.step(agent.memory.actions[-1])\n",
        "    \n",
        "    #4: Get next state\n",
        "    next_frame = resize_frame(next_frame)\n",
        "    new_state = [agent.memory.frames[-3], agent.memory.frames[-2], agent.memory.frames[-1], next_frame]\n",
        "    new_state = np.moveaxis(new_state,0,2)/255 #We have to do this to get it into keras's goofy format of [batch_size,rows,columns,channels]\n",
        "    new_state = np.expand_dims(new_state,0) #^^^\n",
        "    \n",
        "    #5: Get next action, using next state\n",
        "    next_action = agent.get_action(new_state)\n",
        "\n",
        "    #6: If game is over, return the score\n",
        "    if next_frame_terminal:\n",
        "        agent.memory.add_experience(next_frame, next_frames_reward, next_action, next_frame_terminal)\n",
        "        return (score + next_frames_reward),True\n",
        "\n",
        "    #7: Now we add the next experience to memory\n",
        "    agent.memory.add_experience(next_frame, next_frames_reward, next_action, next_frame_terminal)\n",
        "\n",
        "    #8: If we are trying to debug this then render\n",
        "    if debug:\n",
        "        env.render()\n",
        "\n",
        "    #9: If the threshold memory is satisfied, make the agent learn from memory\n",
        "    if len(agent.memory.frames) > agent.starting_mem_len:\n",
        "        agent.learn(debug)\n",
        "\n",
        "    return (score + next_frames_reward),False\n",
        "\n",
        "def play_episode(name, env, agent, debug = False):\n",
        "    initialize_new_game(name, env, agent)\n",
        "    done = False\n",
        "    score = 0\n",
        "    while True:\n",
        "        score,done = take_step(name,env,agent,score, debug)\n",
        "        if done:\n",
        "            break\n",
        "    return score\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential, clone_model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self,possible_actions,starting_mem_len,max_mem_len,starting_epsilon,learn_rate, starting_lives = 5, debug = False):\n",
        "        self.memory = Memory(max_mem_len)\n",
        "        self.possible_actions = possible_actions\n",
        "        self.epsilon = starting_epsilon\n",
        "        self.epsilon_decay = .9/100000\n",
        "        self.epsilon_min = .05\n",
        "        self.gamma = .95\n",
        "        self.learn_rate = learn_rate\n",
        "        self.model = self._build_model()\n",
        "        self.model_target = clone_model(self.model)\n",
        "        self.total_timesteps = 0\n",
        "        self.lives = starting_lives #this parameter does not apply to pong\n",
        "        self.starting_mem_len = starting_mem_len\n",
        "        self.learns = 0\n",
        "\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Input((84,84,4)))\n",
        "        model.add(Conv2D(filters = 32,kernel_size = (8,8),strides = 4,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
        "        model.add(Conv2D(filters = 64,kernel_size = (4,4),strides = 2,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
        "        model.add(Conv2D(filters = 64,kernel_size = (3,3),strides = 1,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512,activation = 'relu', kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
        "        model.add(Dense(len(self.possible_actions), activation = 'linear'))\n",
        "        optimizer = Adam(self.learn_rate)\n",
        "        model.compile(optimizer, loss=tf.keras.losses.Huber())\n",
        "        model.summary()\n",
        "        print('\\nAgent Initialized\\n')\n",
        "        return model\n",
        "\n",
        "    def get_action(self,state):\n",
        "        \"\"\"Explore\"\"\"\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return random.sample(self.possible_actions,1)[0]\n",
        "\n",
        "        \"\"\"Do Best Acton\"\"\"\n",
        "        a_index = np.argmax(self.model.predict(state))\n",
        "        return self.possible_actions[a_index]\n",
        "\n",
        "    def _index_valid(self,index):\n",
        "        if self.memory.done_flags[index-3] or self.memory.done_flags[index-2] or self.memory.done_flags[index-1] or self.memory.done_flags[index]:\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "\n",
        "    def learn(self,debug = False):\n",
        "        \"\"\"we want the output[a] to be R_(t+1) + Qmax_(t+1).\"\"\"\n",
        "        \"\"\"So target for taking action 1 should be [output[0], R_(t+1) + Qmax_(t+1), output[2]]\"\"\"\n",
        "\n",
        "        \"\"\"First we need 32 random valid indicies\"\"\"\n",
        "        states = []\n",
        "        next_states = []\n",
        "        actions_taken = []\n",
        "        next_rewards = []\n",
        "        next_done_flags = []\n",
        "\n",
        "        while len(states) < 32:\n",
        "            index = np.random.randint(4,len(self.memory.frames) - 1)\n",
        "            if self._index_valid(index):\n",
        "                state = [self.memory.frames[index-3], self.memory.frames[index-2], self.memory.frames[index-1], self.memory.frames[index]]\n",
        "                state = np.moveaxis(state,0,2)/255\n",
        "                next_state = [self.memory.frames[index-2], self.memory.frames[index-1], self.memory.frames[index], self.memory.frames[index+1]]\n",
        "                next_state = np.moveaxis(next_state,0,2)/255\n",
        "\n",
        "                states.append(state)\n",
        "                next_states.append(next_state)\n",
        "                actions_taken.append(self.memory.actions[index])\n",
        "                next_rewards.append(self.memory.rewards[index+1])\n",
        "                next_done_flags.append(self.memory.done_flags[index+1])\n",
        "\n",
        "        \"\"\"Now we get the ouputs from our model, and the target model. We need this for our target in the error function\"\"\"\n",
        "        labels = self.model.predict(np.array(states))\n",
        "        next_state_values = self.model_target.predict(np.array(next_states))\n",
        "        \n",
        "        \"\"\"Now we define our labels, or what the output should have been\n",
        "           We want the output[action_taken] to be R_(t+1) + Qmax_(t+1) \"\"\"\n",
        "        for i in range(32):\n",
        "            action = self.possible_actions.index(actions_taken[i])\n",
        "            labels[i][action] = next_rewards[i] + (not next_done_flags[i]) * self.gamma * max(next_state_values[i])\n",
        "\n",
        "        \"\"\"Train our model using the states and outputs generated\"\"\"\n",
        "        self.model.fit(np.array(states),labels,batch_size = 32, epochs = 1, verbose = 0)\n",
        "\n",
        "        \"\"\"Decrease epsilon and update how many times our agent has learned\"\"\"\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon -= self.epsilon_decay\n",
        "        self.learns += 1\n",
        "        \n",
        "        \"\"\"Every 10000 learned, copy our model weights to our target model\"\"\"\n",
        "        if self.learns % 10000 == 0:\n",
        "            self.model_target.set_weights(self.model.get_weights())\n",
        "            print('\\nTarget model updated')\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "\n",
        "name = 'Pong-v0'\n",
        "\n",
        "agent = Agent(possible_actions=[0,2,3],starting_mem_len=50000,max_mem_len=750000,starting_epsilon = 1, learn_rate = .00025)\n",
        "env = make_env(name,agent)\n",
        "\n",
        "last_100_avg = [-21]\n",
        "scores = deque(maxlen = 100)\n",
        "max_score = -21\n",
        "\n",
        "\"\"\" If testing:\n",
        "agent.model.load_weights('recent_weights.hdf5')\n",
        "agent.model_target.load_weights('recent_weights.hdf5')\n",
        "agent.epsilon = 0.0\n",
        "\"\"\"\n",
        "\n",
        "env.reset()\n",
        "\n",
        "for i in range(1000000):\n",
        "    timesteps = agent.total_timesteps\n",
        "    timee = time.time()\n",
        "    score = play_episode(name, env, agent, debug = False) #set debug to true for rendering\n",
        "    scores.append(score)\n",
        "    if score > max_score:\n",
        "        max_score = score\n",
        "\n",
        "    print('\\nEpisode: ' + str(i))\n",
        "    print('Steps: ' + str(agent.total_timesteps - timesteps))\n",
        "    print('Duration: ' + str(time.time() - timee))\n",
        "    print('Score: ' + str(score))\n",
        "    print('Max Score: ' + str(max_score))\n",
        "    print('Epsilon: ' + str(agent.epsilon))\n",
        "\n",
        "    if i%100==0 and i!=0:\n",
        "        last_100_avg.append(sum(scores)/len(scores))\n",
        "        plt.plot(np.arange(0,i+1,100),last_100_avg)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "XmaDSWhLNS59",
        "outputId": "e159c6f5-c162-48ac-f55f-e72b5f9bd802",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 20, 20, 32)        8224      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 9, 9, 64)          32832     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 7, 7, 64)          36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3136)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               1606144   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 1539      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,685,667\n",
            "Trainable params: 1,685,667\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "Agent Initialized\n",
            "\n",
            "\n",
            "Episode: 0\n",
            "Steps: 1301\n",
            "Duration: 4.858506202697754\n",
            "Score: -20.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 1\n",
            "Steps: 1391\n",
            "Duration: 4.787663459777832\n",
            "Score: -20.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 2\n",
            "Steps: 1090\n",
            "Duration: 5.022912263870239\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 3\n",
            "Steps: 1261\n",
            "Duration: 3.8732669353485107\n",
            "Score: -21.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 4\n",
            "Steps: 1482\n",
            "Duration: 4.337517023086548\n",
            "Score: -20.0\n",
            "Max Score: -20.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 5\n",
            "Steps: 1452\n",
            "Duration: 3.015514612197876\n",
            "Score: -19.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 6\n",
            "Steps: 1167\n",
            "Duration: 1.9680676460266113\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 7\n",
            "Steps: 1251\n",
            "Duration: 2.081727981567383\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 8\n",
            "Steps: 1339\n",
            "Duration: 2.230457305908203\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 9\n",
            "Steps: 1097\n",
            "Duration: 1.8346264362335205\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 10\n",
            "Steps: 1523\n",
            "Duration: 2.564910650253296\n",
            "Score: -19.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 11\n",
            "Steps: 1356\n",
            "Duration: 2.290742874145508\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 12\n",
            "Steps: 1198\n",
            "Duration: 2.0076441764831543\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 13\n",
            "Steps: 1418\n",
            "Duration: 2.4457967281341553\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 14\n",
            "Steps: 1189\n",
            "Duration: 2.022980213165283\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 15\n",
            "Steps: 1596\n",
            "Duration: 2.701634168624878\n",
            "Score: -19.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 16\n",
            "Steps: 1600\n",
            "Duration: 2.807872772216797\n",
            "Score: -19.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 17\n",
            "Steps: 1424\n",
            "Duration: 2.465147018432617\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 18\n",
            "Steps: 1539\n",
            "Duration: 2.7429654598236084\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 19\n",
            "Steps: 1098\n",
            "Duration: 1.92698335647583\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 20\n",
            "Steps: 1013\n",
            "Duration: 1.7308597564697266\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 21\n",
            "Steps: 1368\n",
            "Duration: 2.27016282081604\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 22\n",
            "Steps: 1401\n",
            "Duration: 2.35740065574646\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 23\n",
            "Steps: 1101\n",
            "Duration: 1.9405455589294434\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 24\n",
            "Steps: 1267\n",
            "Duration: 2.1454765796661377\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 25\n",
            "Steps: 1109\n",
            "Duration: 1.8216731548309326\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 26\n",
            "Steps: 1488\n",
            "Duration: 2.338179111480713\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 27\n",
            "Steps: 1267\n",
            "Duration: 2.0907561779022217\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 28\n",
            "Steps: 1085\n",
            "Duration: 1.7698709964752197\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 29\n",
            "Steps: 1105\n",
            "Duration: 1.8532133102416992\n",
            "Score: -21.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 30\n",
            "Steps: 1455\n",
            "Duration: 2.394679546356201\n",
            "Score: -20.0\n",
            "Max Score: -19.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 31\n",
            "Steps: 1490\n",
            "Duration: 2.4343760013580322\n",
            "Score: -18.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 32\n",
            "Steps: 1329\n",
            "Duration: 2.1031877994537354\n",
            "Score: -20.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 33\n",
            "Steps: 1708\n",
            "Duration: 2.823561429977417\n",
            "Score: -18.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 34\n",
            "Steps: 1228\n",
            "Duration: 1.9728000164031982\n",
            "Score: -20.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 35\n",
            "Steps: 1666\n",
            "Duration: 2.791300058364868\n",
            "Score: -19.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Episode: 36\n",
            "Steps: 1439\n",
            "Duration: 2.317875623703003\n",
            "Score: -19.0\n",
            "Max Score: -18.0\n",
            "Epsilon: 1\n",
            "\n",
            "Weights saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT4N3qYviUJr"
      },
      "source": [
        "import collections\n",
        "import gym\n",
        "import numpy as np\n",
        "import statistics\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from typing import Any, List, Sequence, Tuple\n",
        "\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make(\"Pong-v0\")\n",
        "\n",
        "# Set seed for experiment reproducibility\n",
        "seed = 42\n",
        "env.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Small epsilon value for stabilizing division operations\n",
        "eps = np.finfo(np.float32).eps.item()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Observation space is: {}\".format(env.observation_space))\n",
        "print(\"Action space is: {}\".format(env.action_space))"
      ],
      "metadata": {
        "id": "HTTT_GjmwcvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9bce86f-8df2-469c-b811-baf9c52a2056"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space is: Box(0, 255, (210, 160, 3), uint8)\n",
            "Action space is: Discrete(6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Render an episode and save as a GIF file\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "\n",
        "# display = Display(visible=0, size=(400, 300))\n",
        "# display.start()\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def resize_frame(frame):\n",
        "    # Resize to 84x84 grayscale\n",
        "    frame = frame[30:-12,5:-4]\n",
        "    frame = np.average(frame,axis = 2)\n",
        "    frame = cv2.resize(frame,(84,84),interpolation = cv2.INTER_NEAREST)\n",
        "    frame = np.array(frame,dtype = np.uint8)\n",
        "    return frame\n",
        "\n",
        "\n",
        "def render_episode(env: gym.Env, max_steps: int): \n",
        "  screen = env.render(mode='rgb_array')\n",
        "  resized_screen = resize_frame(screen)\n",
        "  im = Image.fromarray(resized_screen)\n",
        "\n",
        "  images = [im]\n",
        "  \n",
        "  state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "\n",
        "  for i in range(1, max_steps + 1):\n",
        "    \n",
        "    obs, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
        "    # obs, reward, done, info = env.step(4)\n",
        "    # print(obs.shape)\n",
        "    # print(obs)\n",
        "    # print(reward, done, info)\n",
        "\n",
        "    screen = env.render(mode='rgb_array')\n",
        "    # images.append(Image.fromarray(screen))\n",
        "    resized_screen = resize_frame(screen)\n",
        "    images.append(Image.fromarray(resized_screen))\n",
        "  \n",
        "  return images\n",
        "\n",
        "# Save GIF image\n",
        "images = render_episode(env, 100)\n",
        "image_file = 'pong-v0.gif'\n",
        "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
        "images[0].save(\n",
        "    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n",
        "\n",
        "import tensorflow_docs.vis.embed as embed\n",
        "embed.embed_file(image_file)"
      ],
      "metadata": {
        "id": "YHF9Svfro84x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "f1688c22-5d86-4edc-d2fb-76f4592d0927"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"data:image/gif;base64,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\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOUCe2D0iUJu"
      },
      "source": [
        "## Model\n",
        "\n",
        "The *Actor* and *Critic* will be modeled using one neural network that generates the action probabilities and critic value respectively. This tutorial uses model subclassing to define the model. \n",
        "\n",
        "During the forward pass, the model will take in the state as the input and will output both action probabilities and critic value $V$, which models the state-dependent [value function](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#value-functions). The goal is to train a model that chooses actions based on a policy $\\pi$ that maximizes expected [return](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#reward-and-return).\n",
        "\n",
        "For Cartpole-v0, there are four values representing the state: cart position, cart-velocity, pole angle and pole velocity respectively. The agent can take two actions to push the cart left (0) and right (1) respectively.\n",
        "\n",
        "Refer to [OpenAI Gym's CartPole-v0 wiki page](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf) for more information.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXKbbMC-kmuv"
      },
      "source": [
        "class ActorCritic(tf.keras.Model):\n",
        "  \"\"\"Combined actor-critic network.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self, \n",
        "      num_actions: int, \n",
        "      num_hidden_units: int):\n",
        "    \"\"\"Initialize.\"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n",
        "    # self.actor = layers.Dense(num_actions)\n",
        "    # self.critic = layers.Dense(1)\n",
        "\n",
        "    input_shape = (1, 84, 84)\n",
        "\n",
        "    self.conv = layers.Conv2D(2, 3, activation='relu', input_shape=input_shape[1:])\n",
        "    self.common = layers.Flatten()\n",
        "    self.actor = layers.Dense(num_actions)\n",
        "    self.critic = layers.Dense(1)\n",
        "\n",
        "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    x = self.conv(inputs)\n",
        "    y = self.common(x)\n",
        "    return self.actor(y), self.critic(y)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWyxJgjLn68c"
      },
      "source": [
        "num_actions = env.action_space.n  # 6\n",
        "num_hidden_units = 128\n",
        "\n",
        "model = ActorCritic(num_actions, num_hidden_units)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk92njFziUJw"
      },
      "source": [
        "## Training\n",
        "\n",
        "To train the agent, you will follow these steps:\n",
        "\n",
        "1. Run the agent on the environment to collect training data per episode.\n",
        "2. Compute expected return at each time step.\n",
        "3. Compute the loss for the combined actor-critic model.\n",
        "4. Compute gradients and update network parameters.\n",
        "5. Repeat 1-4 until either success criterion or max episodes has been reached.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2nde2XDs8Gh"
      },
      "source": [
        "### 1. Collecting training data\n",
        "\n",
        "As in supervised learning, in order to train the actor-critic model, you need\n",
        "to have training data. However, in order to collect such data, the model would\n",
        "need to be \"run\" in the \n",
        "\n",
        "Training data is collected for each episode. Then at each time step, the model's forward pass will be run on the environment's state in order to generate action probabilities and the critic value based on the current policy parameterized by the model's weights.\n",
        "\n",
        "The next action will be sampled from the action probabilities generated by the model, which would then be applied to the environment, causing the next state and reward to be generated.\n",
        "\n",
        "This process is implemented in the `run_episode` function, which uses TensorFlow operations so that it can later be compiled into a TensorFlow graph for faster training. Note that `tf.TensorArray`s were used to support Tensor iteration on variable length arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5URrbGlDSAGx"
      },
      "source": [
        "# Wrap OpenAI Gym's `env.step` call as an operation in a TensorFlow function.\n",
        "# This would allow it to be included in a callable TensorFlow graph.\n",
        "\n",
        "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
        "\n",
        "  state, reward, done, _ = env.step(action)\n",
        "  state = resize_frame(state)\n",
        "  return (state.astype(np.float32), \n",
        "          np.array(reward, np.int32), \n",
        "          np.array(done, np.int32))\n",
        "\n",
        "\n",
        "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
        "  return tf.numpy_function(env_step, [action], \n",
        "                           [tf.float32, tf.int32, tf.int32])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4qVRV063Cl9"
      },
      "source": [
        "def run_episode(\n",
        "    initial_state: tf.Tensor,  \n",
        "    model: tf.keras.Model, \n",
        "    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Runs a single episode to collect training data.\"\"\"\n",
        "\n",
        "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
        "\n",
        "  initial_state_shape = initial_state.shape\n",
        "  state = initial_state\n",
        "\n",
        "  for t in tf.range(max_steps):\n",
        "    # Convert state into a batched tensor (batch size = 1)\n",
        "    state = tf.expand_dims(state, 0)\n",
        "  \n",
        "    # Run the model and to get action probabilities and critic value\n",
        "    action_logits_t, value = model(state)\n",
        "    print(action_logits_t, value)\n",
        "  \n",
        "    # Sample next action from the action probability distribution\n",
        "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
        "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
        "\n",
        "    # Store critic values\n",
        "    values = values.write(t, tf.squeeze(value))\n",
        "\n",
        "    # Store log probability of the action chosen\n",
        "    action_probs = action_probs.write(t, action_probs_t[0, action])\n",
        "  \n",
        "    # Apply action to the environment to get next state and reward\n",
        "    state, reward, done = tf_env_step(action)\n",
        "    state.set_shape(initial_state_shape)\n",
        "  \n",
        "    # Store reward\n",
        "    rewards = rewards.write(t, reward)\n",
        "\n",
        "    if tf.cast(done, tf.bool):\n",
        "      break\n",
        "\n",
        "  action_probs = action_probs.stack()\n",
        "  values = values.stack()\n",
        "  rewards = rewards.stack()\n",
        "  \n",
        "  return action_probs, values, rewards"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBnIHdz22dIx"
      },
      "source": [
        "### 2. Computing expected returns\n",
        "\n",
        "The sequence of rewards for each timestep $t$, $\\{r_{t}\\}^{T}_{t=1}$ collected during one episode is converted into a sequence of expected returns $\\{G_{t}\\}^{T}_{t=1}$ in which the sum of rewards is taken from the current timestep $t$ to $T$ and each reward is multiplied with an exponentially decaying discount factor $\\gamma$:\n",
        "\n",
        "$$G_{t} = \\sum^{T}_{t'=t} \\gamma^{t'-t}r_{t'}$$\n",
        "\n",
        "Since $\\gamma\\in(0,1)$, rewards further out from the current timestep are given less weight.\n",
        "\n",
        "Intuitively, expected return simply implies that rewards now are better than rewards later. In a mathematical sense, it is to ensure that the sum of the rewards converges.\n",
        "\n",
        "To stabilize training, the resulting sequence of returns is also standardized (i.e. to have zero mean and unit standard deviation).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpEwFyl315dl"
      },
      "source": [
        "def get_expected_return(\n",
        "    rewards: tf.Tensor, \n",
        "    gamma: float, \n",
        "    standardize: bool = True) -> tf.Tensor:\n",
        "  \"\"\"Compute expected returns per timestep.\"\"\"\n",
        "\n",
        "  n = tf.shape(rewards)[0]\n",
        "  returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
        "\n",
        "  # Start from the end of `rewards` and accumulate reward sums\n",
        "  # into the `returns` array\n",
        "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
        "  discounted_sum = tf.constant(0.0)\n",
        "  discounted_sum_shape = discounted_sum.shape\n",
        "  for i in tf.range(n):\n",
        "    reward = rewards[i]\n",
        "    discounted_sum = reward + gamma * discounted_sum\n",
        "    discounted_sum.set_shape(discounted_sum_shape)\n",
        "    returns = returns.write(i, discounted_sum)\n",
        "  returns = returns.stack()[::-1]\n",
        "\n",
        "  if standardize:\n",
        "    returns = ((returns - tf.math.reduce_mean(returns)) / \n",
        "               (tf.math.reduce_std(returns) + eps))\n",
        "\n",
        "  return returns"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hrPLrgGxlvb"
      },
      "source": [
        "### 3. The actor-critic loss\n",
        "\n",
        "Since a hybrid actor-critic model is used, the chosen loss function is a combination of actor and critic losses for training, as shown below:\n",
        "\n",
        "$$L = L_{actor} + L_{critic}$$\n",
        "\n",
        "#### Actor loss\n",
        "\n",
        "The actor loss is based on [policy gradients with the critic as a state dependent baseline](https://www.youtube.com/watch?v=EKqxumCuAAY&t=62m23s) and computed with single-sample (per-episode) estimates.\n",
        "\n",
        "$$L_{actor} = -\\sum^{T}_{t=1} \\log\\pi_{\\theta}(a_{t} | s_{t})[G(s_{t}, a_{t})  - V^{\\pi}_{\\theta}(s_{t})]$$\n",
        "\n",
        "where:\n",
        "- $T$: the number of timesteps per episode, which can vary per episode\n",
        "- $s_{t}$: the state at timestep $t$\n",
        "- $a_{t}$: chosen action at timestep $t$ given state $s$\n",
        "- $\\pi_{\\theta}$: is the policy (actor) parameterized by $\\theta$\n",
        "- $V^{\\pi}_{\\theta}$: is the value function (critic) also parameterized by $\\theta$\n",
        "- $G = G_{t}$: the expected return for a given state, action pair at timestep $t$\n",
        "\n",
        "A negative term is added to the sum since the idea is to maximize the probabilities of actions yielding higher rewards by minimizing the combined loss.\n",
        "\n",
        "<br>\n",
        "\n",
        "##### Advantage\n",
        "\n",
        "The $G - V$ term in our $L_{actor}$ formulation is called the [advantage](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#advantage-functions), which indicates how much better an action is given a particular state over a random action selected according to the policy $\\pi$ for that state.\n",
        "\n",
        "While it's possible to exclude a baseline, this may result in high variance during training. And the nice thing about choosing the critic $V$ as a baseline is that it trained to be as close as possible to $G$, leading to a lower variance.\n",
        "\n",
        "In addition, without the critic, the algorithm would try to increase probabilities for actions taken on a particular state based on expected return, which may not make much of a difference if the relative probabilities between actions remain the same.\n",
        "\n",
        "For instance, suppose that two actions for a given state would yield the same expected return. Without the critic, the algorithm would try to raise the probability of these actions based on the objective $J$. With the critic, it may turn out that there's no advantage ($G - V = 0$) and thus no benefit gained in increasing the actions' probabilities and the algorithm would set the gradients to zero.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Critic loss\n",
        "\n",
        "Training $V$ to be as close possible to $G$ can be set up as a regression problem with the following loss function:\n",
        "\n",
        "$$L_{critic} = L_{\\delta}(G, V^{\\pi}_{\\theta})$$\n",
        "\n",
        "where $L_{\\delta}$ is the [Huber loss](https://en.wikipedia.org/wiki/Huber_loss), which is less sensitive to outliers in data than squared-error loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EXwbEez6n9m"
      },
      "source": [
        "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "def compute_loss(\n",
        "    action_probs: tf.Tensor,  \n",
        "    values: tf.Tensor,  \n",
        "    returns: tf.Tensor) -> tf.Tensor:\n",
        "  \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
        "\n",
        "  advantage = returns - values\n",
        "\n",
        "  action_log_probs = tf.math.log(action_probs)\n",
        "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
        "\n",
        "  critic_loss = huber_loss(values, returns)\n",
        "\n",
        "  return actor_loss + critic_loss"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSYkQOmRfV75"
      },
      "source": [
        "### 4. Defining the training step to update parameters\n",
        "\n",
        "All of the steps above are combined into a training step that is run every episode. All steps leading up to the loss function are executed with the `tf.GradientTape` context to enable automatic differentiation.\n",
        "\n",
        "This tutorial uses the Adam optimizer to apply the gradients to the model parameters.\n",
        "\n",
        "The sum of the undiscounted rewards, `episode_reward`, is also computed in this step. This value will be used later on to evaluate if the success criterion is met.\n",
        "\n",
        "The `tf.function` context is applied to the `train_step` function so that it can be compiled into a callable TensorFlow graph, which can lead to 10x speedup in training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoccrkF3IFCg"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(\n",
        "    initial_state: tf.Tensor, \n",
        "    model: tf.keras.Model, \n",
        "    optimizer: tf.keras.optimizers.Optimizer, \n",
        "    gamma: float, \n",
        "    max_steps_per_episode: int) -> tf.Tensor:\n",
        "  \"\"\"Runs a model training step.\"\"\"\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # Run the model for one episode to collect training data\n",
        "    action_probs, values, rewards = run_episode(\n",
        "        initial_state, model, max_steps_per_episode) \n",
        "\n",
        "    # Calculate expected returns\n",
        "    returns = get_expected_return(rewards, gamma)\n",
        "\n",
        "    # Convert training data to appropriate TF tensor shapes\n",
        "    action_probs, values, returns = [\n",
        "        tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
        "\n",
        "    # Calculating loss values to update our network\n",
        "    loss = compute_loss(action_probs, values, returns)\n",
        "\n",
        "  # Compute the gradients from the loss\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "  # Apply the gradients to the model's parameters\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  episode_reward = tf.math.reduce_sum(rewards)\n",
        "\n",
        "  return episode_reward"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFvZiDoAflGK"
      },
      "source": [
        "### 5. Run the training loop\n",
        "\n",
        "Training is executed by running the training step until either the success criterion or maximum number of episodes is reached.  \n",
        "\n",
        "A running record of episode rewards is kept in a queue. Once 100 trials are reached, the oldest reward is removed at the left (tail) end of the queue and the newest one is added at the head (right). A running sum of the rewards is also maintained for computational efficiency. \n",
        "\n",
        "Depending on your runtime, training can finish in less than a minute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbmBxnzLiUJx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f5e19980-b4e1-457a-9106-19058ba95fc4"
      },
      "source": [
        "%%time\n",
        "\n",
        "min_episodes_criterion = 100\n",
        "max_episodes = 10000\n",
        "max_steps_per_episode = 1000\n",
        "\n",
        "# Cartpole-v0 is considered solved if average reward is >= 195 over 100 \n",
        "# consecutive trials\n",
        "reward_threshold = 20\n",
        "running_reward = 0\n",
        "\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "\n",
        "# Keep last episodes reward\n",
        "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
        "\n",
        "with tqdm.trange(max_episodes) as t:\n",
        "  for i in t:\n",
        "    initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "    initial_state = resize_frame(initial_state)\n",
        "    episode_reward = int(train_step(\n",
        "        initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
        "    \n",
        "    episodes_reward.append(episode_reward)\n",
        "    running_reward = statistics.mean(episodes_reward)\n",
        "  \n",
        "    t.set_description(f'Episode {i}')\n",
        "    t.set_postfix(\n",
        "        episode_reward=episode_reward, running_reward=running_reward)\n",
        "  \n",
        "    # Show average episode reward every 10 episodes\n",
        "    if i % 10 == 0:\n",
        "      pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
        "  \n",
        "    if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
        "        break\n",
        "\n",
        "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-3a83faf9c5c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\nmin_episodes_criterion = 100\\nmax_episodes = 10000\\nmax_steps_per_episode = 1000\\n\\n# Cartpole-v0 is considered solved if average reward is >= 195 over 100 \\n# consecutive trials\\nreward_threshold = 20\\nrunning_reward = 0\\n\\n# Discount factor for future rewards\\ngamma = 0.99\\n\\n# Keep last episodes reward\\nepisodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\\n\\nwith tqdm.trange(max_episodes) as t:\\n  for i in t:\\n    initial_state = tf.constant(env.reset(), dtype=tf.float32)\\n    initial_state = resize_frame(initial_state)\\n    episode_reward = int(train_step(\\n        initial_state, model, optimizer, gamma, max_steps_per_episode))\\n    \\n    episodes_reward.append(episode_reward)\\n    running_reward = statistics.mean(episodes_reward)\\n  \\n    t.set_description(f'Episode {i}')\\n    t.set_postfix(\\n        episode_reward=episode_reward, running_reward=running_reward)\\n  \\n    # Show average episode reward every 10 episodes\\n    if i % 10 == 0:\\n      pass # print(f'Episode {i}: average reward: {avg_reward}')\\n  \\n    if running_reward > reward_threshold and i >= min_episodes_criterion:  \\n        break\\n\\nprint(f'\\\\nSolved at episode {i}: average reward: {running_reward:.2f}!')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"<ipython-input-13-f7d82fbf9b08>\", line 16, in train_step  *\n        action_probs, values, rewards = run_episode(\n    File \"<ipython-input-9-3b602a0304bf>\", line 19, in run_episode  *\n        action_logits_t, value = model(state)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"actor_critic\" (type ActorCritic).\n    \n    in user code:\n    \n        File \"<ipython-input-6-f2a56b40091b>\", line 23, in call  *\n            x = self.conv(inputs)\n        File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 227, in assert_input_compatibility\n            raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n    \n        ValueError: Input 0 of layer \"conv2d\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (1, 84, 84)\n    \n    \n    Call arguments received:\n       inputs=tf.Tensor(shape=(1, 84, 84), dtype=uint8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru8BEwS1EmAv"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "After training, it would be good to visualize how the model performs in the  You can run the cells below to generate a GIF animation of one episode run of the model. Note that additional packages need to be installed for OpenAI Gym to render the environment's images correctly in Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbIMMkfmRHyC"
      },
      "source": [
        "# Render an episode and save as a GIF file\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "\n",
        "def render_episode(env: gym.Env, model: tf.keras.Model, max_steps: int): \n",
        "  screen = env.render(mode='rgb_array')\n",
        "  im = Image.fromarray(screen)\n",
        "\n",
        "  images = [im]\n",
        "  \n",
        "  state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "  for i in range(1, max_steps + 1):\n",
        "    state = tf.expand_dims(state, 0)\n",
        "    action_probs, _ = model(state)\n",
        "    action = np.argmax(np.squeeze(action_probs))\n",
        "\n",
        "    state, _, done, _ = env.step(action)\n",
        "    state = tf.constant(state, dtype=tf.float32)\n",
        "\n",
        "    # Render screen every 10 steps\n",
        "    if i % 10 == 0:\n",
        "      screen = env.render(mode='rgb_array')\n",
        "      images.append(Image.fromarray(screen))\n",
        "  \n",
        "    if done:\n",
        "      break\n",
        "  \n",
        "  return images\n",
        "\n",
        "\n",
        "# Save GIF image\n",
        "images = render_episode(env, model, max_steps_per_episode)\n",
        "image_file = 'cartpole-v0.gif'\n",
        "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
        "images[0].save(\n",
        "    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLd720SejKmf"
      },
      "source": [
        "import tensorflow_docs.vis.embed as embed\n",
        "embed.embed_file(image_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnq9Hzo1Po6X"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "This tutorial demonstrated how to implement the actor-critic method using Tensorflow.\n",
        "\n",
        "As a next step, you could try training a model on a different environment in OpenAI Gym. \n",
        "\n",
        "For additional information regarding actor-critic methods and the Cartpole-v0 problem, you may refer to the following resources:\n",
        "\n",
        "- [Actor Critic Method](https://hal.inria.fr/hal-00840470/document)\n",
        "- [Actor Critic Lecture (CAL)](https://www.youtube.com/watch?v=EKqxumCuAAY&list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&index=7&t=0s)\n",
        "- [Cartpole learning control problem \\[Barto, et al. 1983\\]](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf) \n",
        "\n",
        "For more reinforcement learning examples in TensorFlow, you can check the following resources:\n",
        "- [Reinforcement learning code examples (keras.io)](https://keras.io/examples/rl/)\n",
        "- [TF-Agents reinforcement learning library](https://www.tensorflow.org/agents)\n"
      ]
    }
  ]
}