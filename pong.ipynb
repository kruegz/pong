{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pong.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_jQ1tEQCxwRx"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kruegz/pong/blob/main/pong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jQ1tEQCxwRx"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_sgB_5dx1f1"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p62G8M_viUJp"
      },
      "source": [
        "# Playing Pong with the Actor-Critic Method\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mJ2i6jvZ3sK"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/reinforcement_learning/actor_critic.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/reinforcement_learning/actor_critic.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/reinforcement_learning/actor_critic.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFgN7h_wiUJq"
      },
      "source": [
        "This tutorial demonstrates how to implement the [Actor-Critic](https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf) method using TensorFlow to train an agent on the [Open AI Gym](https://gym.openai.com/) Pong environment.\n",
        "The reader is assumed to have some familiarity with [policy gradient methods](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf) of reinforcement learning. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kA10ZKRR0hi"
      },
      "source": [
        "**Actor-Critic methods**\n",
        "\n",
        "Actor-Critic methods are [temporal difference (TD) learning](https://en.wikipedia.org/wiki/Temporal_difference_learning) methods that represent the policy function independent of the value function. \n",
        "\n",
        "A policy function (or policy) returns a probability distribution over actions that the agent can take based on the given state.\n",
        "A value function determines the expected return for an agent starting at a given state and acting according to a particular policy forever after.\n",
        "\n",
        "In the Actor-Critic method, the policy is referred to as the *actor* that proposes a set of possible actions given a state, and the estimated value function is referred to as the *critic*, which evaluates actions taken by the *actor* based on the given policy.\n",
        "\n",
        "In this tutorial, both the *Actor* and *Critic* will be represented using one neural network with two outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glLwIctHiUJq"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Import necessary packages and configure global settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13l6BbxKhCKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31f6867a-0a21-451f-f88c-8a8d00685dde"
      },
      "source": [
        "!pip install gym\n",
        "!pip install pyglet\n",
        "!pip install atari-py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet) (0.16.0)\n",
            "Requirement already satisfied: atari-py in /usr/local/lib/python3.7/dist-packages (0.2.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from atari-py) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBeQhPi2S4m5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe49d380-0288-4df5-c567-1224489b7c0f"
      },
      "source": [
        "%%bash\n",
        "# Install additional packages for visualization\n",
        "sudo apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1\n",
        "\n",
        "# Download Atari ROMs\n",
        "# https://github.com/openai/atari-py#roms\n",
        "# http://www.atarimania.com/rom_collection_archive_atari_2600_roms.html\n",
        "wget http://www.atarimania.com/roms/Roms.rar\n",
        "unrar e Roms.rar\n",
        "\n",
        "export DISPLAY=localhost:0.0 "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from Roms.rar\n",
            "\n",
            "No files to extract\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "--2021-12-13 20:16:50--  http://www.atarimania.com/roms/Roms.rar\n",
            "Resolving www.atarimania.com (www.atarimania.com)... 195.154.81.199\n",
            "Connecting to www.atarimania.com (www.atarimania.com)|195.154.81.199|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11128004 (11M) [application/x-rar-compressed]\n",
            "Saving to: ‘Roms.rar.1’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  0%  189K 57s\n",
            "    50K .......... .......... .......... .......... ..........  0%  558K 38s\n",
            "   100K .......... .......... .......... .......... ..........  1%  283K 38s\n",
            "   150K .......... .......... .......... .......... ..........  1%  565K 33s\n",
            "   200K .......... .......... .......... .......... ..........  2% 51.7M 26s\n",
            "   250K .......... .......... .......... .......... ..........  2%  567K 25s\n",
            "   300K .......... .......... .......... .......... ..........  3%  565K 24s\n",
            "   350K .......... .......... .......... .......... ..........  3%  565K 23s\n",
            "   400K .......... .......... .......... .......... ..........  4%  566K 23s\n",
            "   450K .......... .......... .......... .......... ..........  4%  103M 20s\n",
            "   500K .......... .......... .......... .......... ..........  5%  564K 20s\n",
            "   550K .......... .......... .......... .......... ..........  5%  566K 20s\n",
            "   600K .......... .......... .......... .......... ..........  5%  564K 19s\n",
            "   650K .......... .......... .......... .......... ..........  6%  212M 18s\n",
            "   700K .......... .......... .......... .......... ..........  6%  565K 18s\n",
            "   750K .......... .......... .......... .......... ..........  7%  565K 18s\n",
            "   800K .......... .......... .......... .......... ..........  7%  565K 18s\n",
            "   850K .......... .......... .......... .......... ..........  8%  568K 18s\n",
            "   900K .......... .......... .......... .......... ..........  8%  103M 17s\n",
            "   950K .......... .......... .......... .......... ..........  9%  558K 17s\n",
            "  1000K .......... .......... .......... .......... ..........  9%  564K 17s\n",
            "  1050K .......... .......... .......... .......... .......... 10%  565K 17s\n",
            "  1100K .......... .......... .......... .......... .......... 10%  220M 16s\n",
            "  1150K .......... .......... .......... .......... .......... 11%  564K 16s\n",
            "  1200K .......... .......... .......... .......... .......... 11%  560K 16s\n",
            "  1250K .......... .......... .......... .......... .......... 11%  567K 16s\n",
            "  1300K .......... .......... .......... .......... .......... 12%  566K 16s\n",
            "  1350K .......... .......... .......... .......... .......... 12%  139M 15s\n",
            "  1400K .......... .......... .......... .......... .......... 13%  552K 15s\n",
            "  1450K .......... .......... .......... .......... .......... 13%  566K 15s\n",
            "  1500K .......... .......... .......... .......... .......... 14%  565K 15s\n",
            "  1550K .......... .......... .......... .......... .......... 14%  269M 14s\n",
            "  1600K .......... .......... .......... .......... .......... 15%  566K 14s\n",
            "  1650K .......... .......... .......... .......... .......... 15%  564K 14s\n",
            "  1700K .......... .......... .......... .......... .......... 16%  564K 14s\n",
            "  1750K .......... .......... .......... .......... .......... 16%  565K 14s\n",
            "  1800K .......... .......... .......... .......... .......... 17% 80.7M 14s\n",
            "  1850K .......... .......... .......... .......... .......... 17%  565K 14s\n",
            "  1900K .......... .......... .......... .......... .......... 17%  566K 14s\n",
            "  1950K .......... .......... .......... .......... .......... 18%  565K 14s\n",
            "  2000K .......... .......... .......... .......... .......... 18%  564K 14s\n",
            "  2050K .......... .......... .......... .......... .......... 19%  155M 13s\n",
            "  2100K .......... .......... .......... .......... .......... 19%  564K 13s\n",
            "  2150K .......... .......... .......... .......... .......... 20%  565K 13s\n",
            "  2200K .......... .......... .......... .......... .......... 20%  563K 13s\n",
            "  2250K .......... .......... .......... .......... .......... 21%  200M 13s\n",
            "  2300K .......... .......... .......... .......... .......... 21%  563K 13s\n",
            "  2350K .......... .......... .......... .......... .......... 22%  565K 13s\n",
            "  2400K .......... .......... .......... .......... .......... 22%  566K 13s\n",
            "  2450K .......... .......... .......... .......... .......... 23%  565K 13s\n",
            "  2500K .......... .......... .......... .......... .......... 23%  111M 12s\n",
            "  2550K .......... .......... .......... .......... .......... 23%  563K 12s\n",
            "  2600K .......... .......... .......... .......... .......... 24%  566K 12s\n",
            "  2650K .......... .......... .......... .......... .......... 24%  565K 12s\n",
            "  2700K .......... .......... .......... .......... .......... 25%  225M 12s\n",
            "  2750K .......... .......... .......... .......... .......... 25%  565K 12s\n",
            "  2800K .......... .......... .......... .......... .......... 26%  566K 12s\n",
            "  2850K .......... .......... .......... .......... .......... 26%  565K 12s\n",
            "  2900K .......... .......... .......... .......... .......... 27%  564K 12s\n",
            "  2950K .......... .......... .......... .......... .......... 27%  184M 12s\n",
            "  3000K .......... .......... .......... .......... .......... 28%  564K 12s\n",
            "  3050K .......... .......... .......... .......... .......... 28%  559K 12s\n",
            "  3100K .......... .......... .......... .......... .......... 28%  565K 12s\n",
            "  3150K .......... .......... .......... .......... .......... 29%  223M 11s\n",
            "  3200K .......... .......... .......... .......... .......... 29%  565K 11s\n",
            "  3250K .......... .......... .......... .......... .......... 30%  566K 11s\n",
            "  3300K .......... .......... .......... .......... .......... 30%  566K 11s\n",
            "  3350K .......... .......... .......... .......... .......... 31%  566K 11s\n",
            "  3400K .......... .......... .......... .......... .......... 31%  171M 11s\n",
            "  3450K .......... .......... .......... .......... .......... 32%  564K 11s\n",
            "  3500K .......... .......... .......... .......... .......... 32%  564K 11s\n",
            "  3550K .......... .......... .......... .......... .......... 33%  563K 11s\n",
            "  3600K .......... .......... .......... .......... .......... 33%  564K 11s\n",
            "  3650K .......... .......... .......... .......... .......... 34% 60.9M 10s\n",
            "  3700K .......... .......... .......... .......... .......... 34%  563K 10s\n",
            "  3750K .......... .......... .......... .......... .......... 34%  559K 10s\n",
            "  3800K .......... .......... .......... .......... .......... 35%  564K 10s\n",
            "  3850K .......... .......... .......... .......... .......... 35%  123M 10s\n",
            "  3900K .......... .......... .......... .......... .......... 36%  563K 10s\n",
            "  3950K .......... .......... .......... .......... .......... 36%  564K 10s\n",
            "  4000K .......... .......... .......... .......... .......... 37%  562K 10s\n",
            "  4050K .......... .......... .......... .......... .......... 37%  563K 10s\n",
            "  4100K .......... .......... .......... .......... .......... 38% 71.8M 10s\n",
            "  4150K .......... .......... .......... .......... .......... 38%  568K 10s\n",
            "  4200K .......... .......... .......... .......... .......... 39%  562K 10s\n",
            "  4250K .......... .......... .......... .......... .......... 39%  565K 10s\n",
            "  4300K .......... .......... .......... .......... .......... 40% 84.6M 9s\n",
            "  4350K .......... .......... .......... .......... .......... 40%  566K 9s\n",
            "  4400K .......... .......... .......... .......... .......... 40%  561K 9s\n",
            "  4450K .......... .......... .......... .......... .......... 41%  564K 9s\n",
            "  4500K .......... .......... .......... .......... .......... 41%  568K 9s\n",
            "  4550K .......... .......... .......... .......... .......... 42%  163M 9s\n",
            "  4600K .......... .......... .......... .......... .......... 42%  563K 9s\n",
            "  4650K .......... .......... .......... .......... .......... 43%  562K 9s\n",
            "  4700K .......... .......... .......... .......... .......... 43%  562K 9s\n",
            "  4750K .......... .......... .......... .......... .......... 44%  101M 9s\n",
            "  4800K .......... .......... .......... .......... .......... 44%  567K 9s\n",
            "  4850K .......... .......... .......... .......... .......... 45%  568K 9s\n",
            "  4900K .......... .......... .......... .......... .......... 45%  564K 9s\n",
            "  4950K .......... .......... .......... .......... .......... 46%  566K 9s\n",
            "  5000K .......... .......... .......... .......... .......... 46% 35.7M 8s\n",
            "  5050K .......... .......... .......... .......... .......... 46%  565K 8s\n",
            "  5100K .......... .......... .......... .......... .......... 47%  559K 8s\n",
            "  5150K .......... .......... .......... .......... .......... 47%  565K 8s\n",
            "  5200K .......... .......... .......... .......... .......... 48%  566K 8s\n",
            "  5250K .......... .......... .......... .......... .......... 48% 69.6M 8s\n",
            "  5300K .......... .......... .......... .......... .......... 49%  563K 8s\n",
            "  5350K .......... .......... .......... .......... .......... 49%  565K 8s\n",
            "  5400K .......... .......... .......... .......... .......... 50%  562K 8s\n",
            "  5450K .......... .......... .......... .......... .......... 50%  271M 8s\n",
            "  5500K .......... .......... .......... .......... .......... 51%  564K 8s\n",
            "  5550K .......... .......... .......... .......... .......... 51%  565K 8s\n",
            "  5600K .......... .......... .......... .......... .......... 51%  565K 8s\n",
            "  5650K .......... .......... .......... .......... .......... 52%  564K 7s\n",
            "  5700K .......... .......... .......... .......... .......... 52% 85.7M 7s\n",
            "  5750K .......... .......... .......... .......... .......... 53%  564K 7s\n",
            "  5800K .......... .......... .......... .......... .......... 53%  563K 7s\n",
            "  5850K .......... .......... .......... .......... .......... 54%  565K 7s\n",
            "  5900K .......... .......... .......... .......... .......... 54%  239M 7s\n",
            "  5950K .......... .......... .......... .......... .......... 55%  566K 7s\n",
            "  6000K .......... .......... .......... .......... .......... 55%  564K 7s\n",
            "  6050K .......... .......... .......... .......... .......... 56%  562K 7s\n",
            "  6100K .......... .......... .......... .......... .......... 56%  563K 7s\n",
            "  6150K .......... .......... .......... .......... .......... 57%  207M 7s\n",
            "  6200K .......... .......... .......... .......... .......... 57%  565K 7s\n",
            "  6250K .......... .......... .......... .......... .......... 57%  563K 7s\n",
            "  6300K .......... .......... .......... .......... .......... 58%  564K 7s\n",
            "  6350K .......... .......... .......... .......... .......... 58%  297M 6s\n",
            "  6400K .......... .......... .......... .......... .......... 59%  563K 6s\n",
            "  6450K .......... .......... .......... .......... .......... 59%  564K 6s\n",
            "  6500K .......... .......... .......... .......... .......... 60%  566K 6s\n",
            "  6550K .......... .......... .......... .......... .......... 60%  566K 6s\n",
            "  6600K .......... .......... .......... .......... .......... 61%  116M 6s\n",
            "  6650K .......... .......... .......... .......... .......... 61%  565K 6s\n",
            "  6700K .......... .......... .......... .......... .......... 62%  563K 6s\n",
            "  6750K .......... .......... .......... .......... .......... 62%  564K 6s\n",
            "  6800K .......... .......... .......... .......... .......... 63%  563K 6s\n",
            "  6850K .......... .......... .......... .......... .......... 63% 75.3M 6s\n",
            "  6900K .......... .......... .......... .......... .......... 63%  562K 6s\n",
            "  6950K .......... .......... .......... .......... .......... 64%  565K 6s\n",
            "  7000K .......... .......... .......... .......... .......... 64%  566K 5s\n",
            "  7050K .......... .......... .......... .......... .......... 65%  269M 5s\n",
            "  7100K .......... .......... .......... .......... .......... 65%  564K 5s\n",
            "  7150K .......... .......... .......... .......... .......... 66%  562K 5s\n",
            "  7200K .......... .......... .......... .......... .......... 66%  562K 5s\n",
            "  7250K .......... .......... .......... .......... .......... 67%  569K 5s\n",
            "  7300K .......... .......... .......... .......... .......... 67% 89.5M 5s\n",
            "  7350K .......... .......... .......... .......... .......... 68%  564K 5s\n",
            "  7400K .......... .......... .......... .......... .......... 68%  565K 5s\n",
            "  7450K .......... .......... .......... .......... .......... 69%  567K 5s\n",
            "  7500K .......... .......... .......... .......... .......... 69%  125M 5s\n",
            "  7550K .......... .......... .......... .......... .......... 69%  564K 5s\n",
            "  7600K .......... .......... .......... .......... .......... 70%  564K 5s\n",
            "  7650K .......... .......... .......... .......... .......... 70%  566K 5s\n",
            "  7700K .......... .......... .......... .......... .......... 71%  564K 4s\n",
            "  7750K .......... .......... .......... .......... .......... 71%  113M 4s\n",
            "  7800K .......... .......... .......... .......... .......... 72%  565K 4s\n",
            "  7850K .......... .......... .......... .......... .......... 72%  562K 4s\n",
            "  7900K .......... .......... .......... .......... .......... 73%  567K 4s\n",
            "  7950K .......... .......... .......... .......... .......... 73% 95.2M 4s\n",
            "  8000K .......... .......... .......... .......... .......... 74%  564K 4s\n",
            "  8050K .......... .......... .......... .......... .......... 74%  566K 4s\n",
            "  8100K .......... .......... .......... .......... .......... 74%  562K 4s\n",
            "  8150K .......... .......... .......... .......... .......... 75%  563K 4s\n",
            "  8200K .......... .......... .......... .......... .......... 75% 84.0M 4s\n",
            "  8250K .......... .......... .......... .......... .......... 76%  564K 4s\n",
            "  8300K .......... .......... .......... .......... .......... 76%  566K 4s\n",
            "  8350K .......... .......... .......... .......... .......... 77%  566K 4s\n",
            "  8400K .......... .......... .......... .......... .......... 77%  566K 3s\n",
            "  8450K .......... .......... .......... .......... .......... 78%  129M 3s\n",
            "  8500K .......... .......... .......... .......... .......... 78%  564K 3s\n",
            "  8550K .......... .......... .......... .......... .......... 79%  565K 3s\n",
            "  8600K .......... .......... .......... .......... .......... 79%  561K 3s\n",
            "  8650K .......... .......... .......... .......... .......... 80%  310M 3s\n",
            "  8700K .......... .......... .......... .......... .......... 80%  565K 3s\n",
            "  8750K .......... .......... .......... .......... .......... 80%  554K 3s\n",
            "  8800K .......... .......... .......... .......... .......... 81%  566K 3s\n",
            "  8850K .......... .......... .......... .......... .......... 81%  564K 3s\n",
            "  8900K .......... .......... .......... .......... .......... 82% 63.4M 3s\n",
            "  8950K .......... .......... .......... .......... .......... 82%  565K 3s\n",
            "  9000K .......... .......... .......... .......... .......... 83%  565K 3s\n",
            "  9050K .......... .......... .......... .......... .......... 83%  566K 3s\n",
            "  9100K .......... .......... .......... .......... .......... 84%  179M 2s\n",
            "  9150K .......... .......... .......... .......... .......... 84%  565K 2s\n",
            "  9200K .......... .......... .......... .......... .......... 85%  565K 2s\n",
            "  9250K .......... .......... .......... .......... .......... 85%  563K 2s\n",
            "  9300K .......... .......... .......... .......... .......... 86%  565K 2s\n",
            "  9350K .......... .......... .......... .......... .......... 86% 96.5M 2s\n",
            "  9400K .......... .......... .......... .......... .......... 86%  563K 2s\n",
            "  9450K .......... .......... .......... .......... .......... 87%  564K 2s\n",
            "  9500K .......... .......... .......... .......... .......... 87%  565K 2s\n",
            "  9550K .......... .......... .......... .......... .......... 88%  257M 2s\n",
            "  9600K .......... .......... .......... .......... .......... 88%  564K 2s\n",
            "  9650K .......... .......... .......... .......... .......... 89%  562K 2s\n",
            "  9700K .......... .......... .......... .......... .......... 89%  565K 2s\n",
            "  9750K .......... .......... .......... .......... .......... 90%  565K 2s\n",
            "  9800K .......... .......... .......... .......... .......... 90%  164M 1s\n",
            "  9850K .......... .......... .......... .......... .......... 91%  565K 1s\n",
            "  9900K .......... .......... .......... .......... .......... 91%  563K 1s\n",
            "  9950K .......... .......... .......... .......... .......... 92%  563K 1s\n",
            " 10000K .......... .......... .......... .......... .......... 92%  564K 1s\n",
            " 10050K .......... .......... .......... .......... .......... 92% 96.6M 1s\n",
            " 10100K .......... .......... .......... .......... .......... 93%  566K 1s\n",
            " 10150K .......... .......... .......... .......... .......... 93%  563K 1s\n",
            " 10200K .......... .......... .......... .......... .......... 94%  564K 1s\n",
            " 10250K .......... .......... .......... .......... .......... 94%  210M 1s\n",
            " 10300K .......... .......... .......... .......... .......... 95%  565K 1s\n",
            " 10350K .......... .......... .......... .......... .......... 95%  567K 1s\n",
            " 10400K .......... .......... .......... .......... .......... 96%  564K 1s\n",
            " 10450K .......... .......... .......... .......... .......... 96%  565K 1s\n",
            " 10500K .......... .......... .......... .......... .......... 97%  128M 0s\n",
            " 10550K .......... .......... .......... .......... .......... 97%  565K 0s\n",
            " 10600K .......... .......... .......... .......... .......... 98%  565K 0s\n",
            " 10650K .......... .......... .......... .......... .......... 98%  566K 0s\n",
            " 10700K .......... .......... .......... .......... .......... 98%  307M 0s\n",
            " 10750K .......... .......... .......... .......... .......... 99%  563K 0s\n",
            " 10800K .......... .......... .......... .......... .......... 99%  561K 0s\n",
            " 10850K .......... .......                                    100%  350M=15s\n",
            "\n",
            "2021-12-13 20:17:06 (708 KB/s) - ‘Roms.rar.1’ saved [11128004/11128004]\n",
            "\n",
            "\n",
            "\n",
            "Would you like to replace the existing file HC ROMS.zip\n",
            "11826711 bytes, modified on 2019-12-22 11:24\n",
            "with a new one\n",
            "11826711 bytes, modified on 2019-12-22 11:24\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit \n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "ls -la\n",
        "python -m atari_py.import_roms .\n",
        "ls -la"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS_5KKfX_tH8",
        "outputId": "347dd28c-36b6-4886-e204-bde10ed2c8d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 41296\n",
            "drwxr-xr-x 1 root root     4096 Dec 13 20:16 .\n",
            "drwxr-xr-x 1 root root     4096 Dec 13 19:45 ..\n",
            "drwxr-xr-x 4 root root     4096 Dec  3 14:33 .config\n",
            "-rw-r--r-- 1 root root 11826711 Dec 22  2019 HC ROMS.zip\n",
            "-rw-r--r-- 1 root root 11128004 Dec 22  2019 Roms.rar\n",
            "-rw-r--r-- 1 root root 11128004 Dec 22  2019 Roms.rar.1\n",
            "-rw-r--r-- 1 root root  8181588 Dec 22  2019 ROMS.zip\n",
            "drwxr-xr-x 1 root root     4096 Dec  3 14:33 sample_data\n",
            "copying adventure.bin from ROMS/Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying air_raid.bin from ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying alien.bin from ROMS/Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying amidar.bin from ROMS/Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying assault.bin from ROMS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying asterix.bin from ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n",
            "copying asteroids.bin from ROMS/Asteroids (1981) (Atari, Brad Stewart - Sears) (CX2649 - 49-75163) [no copyright] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying atlantis.bin from ROMS/Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying bank_heist.bin from ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
            "copying battle_zone.bin from ROMS/Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying beam_rider.bin from ROMS/Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying berzerk.bin from ROMS/Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying bowling.bin from ROMS/Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying boxing.bin from ROMS/Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying breakout.bin from ROMS/Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying carnival.bin from ROMS/Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying centipede.bin from ROMS/Centipede (1983) (Atari - GCC) (CX2676) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying chopper_command.bin from ROMS/Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying crazy_climber.bin from ROMS/Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying defender.bin from ROMS/Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying demon_attack.bin from ROMS/Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying donkey_kong.bin from ROMS/Donkey Kong (1982) (Coleco - Woodside Design Associates - Imaginative Systems Software, Garry Kitchen) (2451) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying double_dunk.bin from ROMS/Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying elevator_action.bin from ROMS/Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying enduro.bin from ROMS/Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying fishing_derby.bin from ROMS/Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying freeway.bin from ROMS/Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying frogger.bin from ROMS/Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying frostbite.bin from ROMS/Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying galaxian.bin from ROMS/Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying gopher.bin from ROMS/Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying gravitar.bin from ROMS/Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying hero.bin from ROMS/H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying ice_hockey.bin from ROMS/Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying jamesbond.bin from ROMS/James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying journey_escape.bin from ROMS/Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying kaboom.bin from ROMS/Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying kangaroo.bin from ROMS/Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying keystone_kapers.bin from ROMS/Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying king_kong.bin from ROMS/King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying koolaid.bin from ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying krull.bin from ROMS/Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying kung_fu_master.bin from ROMS/Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying laser_gates.bin from ROMS/Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying lost_luggage.bin from ROMS/Lost Luggage (Airport Mayhem) (1982) (Apollo - Games by Apollo, Larry Minor, Ernie Runyon, Ed Salvo) (AP-2004) [no opening scene] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying montezuma_revenge.bin from ROMS/Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying mr_do.bin from ROMS/Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying ms_pacman.bin from ROMS/Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying name_this_game.bin from ROMS/Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying pacman.bin from ROMS/Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying phoenix.bin from ROMS/Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying video_pinball.bin from ROMS/Pinball (AKA Video Pinball) (Zellers).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying pitfall.bin from ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying pooyan.bin from ROMS/Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying private_eye.bin from ROMS/Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying qbert.bin from ROMS/Q-bert (1983) (Parker Brothers - Western Technologies, Dave Hampton, Tom Sloper) (PB5360) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying riverraid.bin from ROMS/River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying road_runner.bin from patched version of ROMS/Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying robotank.bin from ROMS/Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying seaquest.bin from ROMS/Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying sir_lancelot.bin from ROMS/Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n",
            "copying skiing.bin from ROMS/Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying solaris.bin from ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying space_invaders.bin from ROMS/Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying star_gunner.bin from ROMS/Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying surround.bin from ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying tennis.bin from ROMS/Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying time_pilot.bin from ROMS/Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying trondead.bin from ROMS/TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying tutankham.bin from ROMS/Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying up_n_down.bin from ROMS/Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying venture.bin from ROMS/Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying pong.bin from ROMS/Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying wizard_of_wor.bin from ROMS/Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying yars_revenge.bin from ROMS/Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying zaxxon.bin from ROMS/Zaxxon (1983) (Coleco) (2454) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n",
            "total 41296\n",
            "drwxr-xr-x 1 root root     4096 Dec 13 20:16 .\n",
            "drwxr-xr-x 1 root root     4096 Dec 13 19:45 ..\n",
            "drwxr-xr-x 4 root root     4096 Dec  3 14:33 .config\n",
            "-rw-r--r-- 1 root root 11826711 Dec 22  2019 HC ROMS.zip\n",
            "-rw-r--r-- 1 root root 11128004 Dec 22  2019 Roms.rar\n",
            "-rw-r--r-- 1 root root 11128004 Dec 22  2019 Roms.rar.1\n",
            "-rw-r--r-- 1 root root  8181588 Dec 22  2019 ROMS.zip\n",
            "drwxr-xr-x 1 root root     4096 Dec  3 14:33 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT4N3qYviUJr"
      },
      "source": [
        "import collections\n",
        "import gym\n",
        "import numpy as np\n",
        "import statistics\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from typing import Any, List, Sequence, Tuple\n",
        "\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make(\"Pong-v0\")\n",
        "\n",
        "# Set seed for experiment reproducibility\n",
        "seed = 42\n",
        "env.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Small epsilon value for stabilizing division operations\n",
        "eps = np.finfo(np.float32).eps.item()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(type(env.action_space))\n",
        "# print(env.action_space)\n",
        "# for _ in range(10): \n",
        "#   print(env.action_space.sample())"
      ],
      "metadata": {
        "id": "HTTT_GjmwcvP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Render an episode and save as a GIF file\n",
        "\n",
        "# from IPython import display as ipythondisplay\n",
        "# from PIL import Image\n",
        "# from pyvirtualdisplay import Display\n",
        "\n",
        "\n",
        "# display = Display(visible=0, size=(400, 300))\n",
        "# display.start()\n",
        "\n",
        "\n",
        "# def render_episode(env: gym.Env, max_steps: int): \n",
        "#   screen = env.render(mode='rgb_array')\n",
        "#   im = Image.fromarray(screen)\n",
        "\n",
        "#   images = [im]\n",
        "  \n",
        "#   state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "\n",
        "#   for i in range(1, max_steps + 1):\n",
        "    \n",
        "#     obs, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
        "#     # obs, reward, done, info = env.step(4)\n",
        "#     # print(obs.shape)\n",
        "#     # print(obs)\n",
        "#     # print(reward, done, info)\n",
        "\n",
        "#     screen = env.render(mode='rgb_array')\n",
        "#     images.append(Image.fromarray(screen))\n",
        "  \n",
        "#   return images\n",
        "\n",
        "\n",
        "# # Save GIF image\n",
        "# images = render_episode(env, 1)\n",
        "# image_file = 'pong-v0.gif'\n",
        "# # loop=0: loop forever, duration=1: play each frame for 1ms\n",
        "# images[0].save(\n",
        "#     image_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n",
        "\n",
        "# import tensorflow_docs.vis.embed as embed\n",
        "# embed.embed_file(image_file)"
      ],
      "metadata": {
        "id": "YHF9Svfro84x"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOUCe2D0iUJu"
      },
      "source": [
        "## Model\n",
        "\n",
        "The *Actor* and *Critic* will be modeled using one neural network that generates the action probabilities and critic value respectively. This tutorial uses model subclassing to define the model. \n",
        "\n",
        "During the forward pass, the model will take in the state as the input and will output both action probabilities and critic value $V$, which models the state-dependent [value function](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#value-functions). The goal is to train a model that chooses actions based on a policy $\\pi$ that maximizes expected [return](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#reward-and-return).\n",
        "\n",
        "For Cartpole-v0, there are four values representing the state: cart position, cart-velocity, pole angle and pole velocity respectively. The agent can take two actions to push the cart left (0) and right (1) respectively.\n",
        "\n",
        "Refer to [OpenAI Gym's CartPole-v0 wiki page](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf) for more information.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXKbbMC-kmuv"
      },
      "source": [
        "class ActorCritic(tf.keras.Model):\n",
        "  \"\"\"Combined actor-critic network.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self, \n",
        "      num_actions: int, \n",
        "      num_hidden_units: int):\n",
        "    \"\"\"Initialize.\"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n",
        "    # self.actor = layers.Dense(num_actions)\n",
        "    # self.critic = layers.Dense(1)\n",
        "\n",
        "    input_shape = (1, 210, 160, 3)\n",
        "\n",
        "    self.conv = layers.Conv2D(2, 3, activation='relu', input_shape=input_shape[1:])\n",
        "    self.common = layers.Flatten()\n",
        "    self.actor = layers.Dense(num_actions)\n",
        "    self.critic = layers.Dense(1)\n",
        "\n",
        "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    x = self.conv(inputs)\n",
        "    y = self.common(x)\n",
        "    return self.actor(y), self.critic(y)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWyxJgjLn68c"
      },
      "source": [
        "num_actions = env.action_space.n  # 6\n",
        "num_hidden_units = 128\n",
        "\n",
        "model = ActorCritic(num_actions, num_hidden_units)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk92njFziUJw"
      },
      "source": [
        "## Training\n",
        "\n",
        "To train the agent, you will follow these steps:\n",
        "\n",
        "1. Run the agent on the environment to collect training data per episode.\n",
        "2. Compute expected return at each time step.\n",
        "3. Compute the loss for the combined actor-critic model.\n",
        "4. Compute gradients and update network parameters.\n",
        "5. Repeat 1-4 until either success criterion or max episodes has been reached.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2nde2XDs8Gh"
      },
      "source": [
        "### 1. Collecting training data\n",
        "\n",
        "As in supervised learning, in order to train the actor-critic model, you need\n",
        "to have training data. However, in order to collect such data, the model would\n",
        "need to be \"run\" in the environment.\n",
        "\n",
        "Training data is collected for each episode. Then at each time step, the model's forward pass will be run on the environment's state in order to generate action probabilities and the critic value based on the current policy parameterized by the model's weights.\n",
        "\n",
        "The next action will be sampled from the action probabilities generated by the model, which would then be applied to the environment, causing the next state and reward to be generated.\n",
        "\n",
        "This process is implemented in the `run_episode` function, which uses TensorFlow operations so that it can later be compiled into a TensorFlow graph for faster training. Note that `tf.TensorArray`s were used to support Tensor iteration on variable length arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5URrbGlDSAGx"
      },
      "source": [
        "# Wrap OpenAI Gym's `env.step` call as an operation in a TensorFlow function.\n",
        "# This would allow it to be included in a callable TensorFlow graph.\n",
        "\n",
        "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
        "\n",
        "  state, reward, done, _ = env.step(action)\n",
        "  return (state.astype(np.float32), \n",
        "          np.array(reward, np.int32), \n",
        "          np.array(done, np.int32))\n",
        "\n",
        "\n",
        "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
        "  return tf.numpy_function(env_step, [action], \n",
        "                           [tf.float32, tf.int32, tf.int32])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4qVRV063Cl9"
      },
      "source": [
        "def run_episode(\n",
        "    initial_state: tf.Tensor,  \n",
        "    model: tf.keras.Model, \n",
        "    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Runs a single episode to collect training data.\"\"\"\n",
        "\n",
        "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
        "\n",
        "  initial_state_shape = initial_state.shape\n",
        "  state = initial_state\n",
        "\n",
        "  for t in tf.range(max_steps):\n",
        "    # Convert state into a batched tensor (batch size = 1)\n",
        "    state = tf.expand_dims(state, 0)\n",
        "  \n",
        "    # Run the model and to get action probabilities and critic value\n",
        "    action_logits_t, value = model(state)\n",
        "    print(action_logits_t, value)\n",
        "  \n",
        "    # Sample next action from the action probability distribution\n",
        "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
        "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
        "\n",
        "    # Store critic values\n",
        "    values = values.write(t, tf.squeeze(value))\n",
        "\n",
        "    # Store log probability of the action chosen\n",
        "    action_probs = action_probs.write(t, action_probs_t[0, action])\n",
        "  \n",
        "    # Apply action to the environment to get next state and reward\n",
        "    state, reward, done = tf_env_step(action)\n",
        "    state.set_shape(initial_state_shape)\n",
        "  \n",
        "    # Store reward\n",
        "    rewards = rewards.write(t, reward)\n",
        "\n",
        "    if tf.cast(done, tf.bool):\n",
        "      break\n",
        "\n",
        "  action_probs = action_probs.stack()\n",
        "  values = values.stack()\n",
        "  rewards = rewards.stack()\n",
        "  \n",
        "  return action_probs, values, rewards"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBnIHdz22dIx"
      },
      "source": [
        "### 2. Computing expected returns\n",
        "\n",
        "The sequence of rewards for each timestep $t$, $\\{r_{t}\\}^{T}_{t=1}$ collected during one episode is converted into a sequence of expected returns $\\{G_{t}\\}^{T}_{t=1}$ in which the sum of rewards is taken from the current timestep $t$ to $T$ and each reward is multiplied with an exponentially decaying discount factor $\\gamma$:\n",
        "\n",
        "$$G_{t} = \\sum^{T}_{t'=t} \\gamma^{t'-t}r_{t'}$$\n",
        "\n",
        "Since $\\gamma\\in(0,1)$, rewards further out from the current timestep are given less weight.\n",
        "\n",
        "Intuitively, expected return simply implies that rewards now are better than rewards later. In a mathematical sense, it is to ensure that the sum of the rewards converges.\n",
        "\n",
        "To stabilize training, the resulting sequence of returns is also standardized (i.e. to have zero mean and unit standard deviation).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpEwFyl315dl"
      },
      "source": [
        "def get_expected_return(\n",
        "    rewards: tf.Tensor, \n",
        "    gamma: float, \n",
        "    standardize: bool = True) -> tf.Tensor:\n",
        "  \"\"\"Compute expected returns per timestep.\"\"\"\n",
        "\n",
        "  n = tf.shape(rewards)[0]\n",
        "  returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
        "\n",
        "  # Start from the end of `rewards` and accumulate reward sums\n",
        "  # into the `returns` array\n",
        "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
        "  discounted_sum = tf.constant(0.0)\n",
        "  discounted_sum_shape = discounted_sum.shape\n",
        "  for i in tf.range(n):\n",
        "    reward = rewards[i]\n",
        "    discounted_sum = reward + gamma * discounted_sum\n",
        "    discounted_sum.set_shape(discounted_sum_shape)\n",
        "    returns = returns.write(i, discounted_sum)\n",
        "  returns = returns.stack()[::-1]\n",
        "\n",
        "  if standardize:\n",
        "    returns = ((returns - tf.math.reduce_mean(returns)) / \n",
        "               (tf.math.reduce_std(returns) + eps))\n",
        "\n",
        "  return returns"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hrPLrgGxlvb"
      },
      "source": [
        "### 3. The actor-critic loss\n",
        "\n",
        "Since a hybrid actor-critic model is used, the chosen loss function is a combination of actor and critic losses for training, as shown below:\n",
        "\n",
        "$$L = L_{actor} + L_{critic}$$\n",
        "\n",
        "#### Actor loss\n",
        "\n",
        "The actor loss is based on [policy gradients with the critic as a state dependent baseline](https://www.youtube.com/watch?v=EKqxumCuAAY&t=62m23s) and computed with single-sample (per-episode) estimates.\n",
        "\n",
        "$$L_{actor} = -\\sum^{T}_{t=1} \\log\\pi_{\\theta}(a_{t} | s_{t})[G(s_{t}, a_{t})  - V^{\\pi}_{\\theta}(s_{t})]$$\n",
        "\n",
        "where:\n",
        "- $T$: the number of timesteps per episode, which can vary per episode\n",
        "- $s_{t}$: the state at timestep $t$\n",
        "- $a_{t}$: chosen action at timestep $t$ given state $s$\n",
        "- $\\pi_{\\theta}$: is the policy (actor) parameterized by $\\theta$\n",
        "- $V^{\\pi}_{\\theta}$: is the value function (critic) also parameterized by $\\theta$\n",
        "- $G = G_{t}$: the expected return for a given state, action pair at timestep $t$\n",
        "\n",
        "A negative term is added to the sum since the idea is to maximize the probabilities of actions yielding higher rewards by minimizing the combined loss.\n",
        "\n",
        "<br>\n",
        "\n",
        "##### Advantage\n",
        "\n",
        "The $G - V$ term in our $L_{actor}$ formulation is called the [advantage](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#advantage-functions), which indicates how much better an action is given a particular state over a random action selected according to the policy $\\pi$ for that state.\n",
        "\n",
        "While it's possible to exclude a baseline, this may result in high variance during training. And the nice thing about choosing the critic $V$ as a baseline is that it trained to be as close as possible to $G$, leading to a lower variance.\n",
        "\n",
        "In addition, without the critic, the algorithm would try to increase probabilities for actions taken on a particular state based on expected return, which may not make much of a difference if the relative probabilities between actions remain the same.\n",
        "\n",
        "For instance, suppose that two actions for a given state would yield the same expected return. Without the critic, the algorithm would try to raise the probability of these actions based on the objective $J$. With the critic, it may turn out that there's no advantage ($G - V = 0$) and thus no benefit gained in increasing the actions' probabilities and the algorithm would set the gradients to zero.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Critic loss\n",
        "\n",
        "Training $V$ to be as close possible to $G$ can be set up as a regression problem with the following loss function:\n",
        "\n",
        "$$L_{critic} = L_{\\delta}(G, V^{\\pi}_{\\theta})$$\n",
        "\n",
        "where $L_{\\delta}$ is the [Huber loss](https://en.wikipedia.org/wiki/Huber_loss), which is less sensitive to outliers in data than squared-error loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EXwbEez6n9m"
      },
      "source": [
        "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "def compute_loss(\n",
        "    action_probs: tf.Tensor,  \n",
        "    values: tf.Tensor,  \n",
        "    returns: tf.Tensor) -> tf.Tensor:\n",
        "  \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
        "\n",
        "  advantage = returns - values\n",
        "\n",
        "  action_log_probs = tf.math.log(action_probs)\n",
        "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
        "\n",
        "  critic_loss = huber_loss(values, returns)\n",
        "\n",
        "  return actor_loss + critic_loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSYkQOmRfV75"
      },
      "source": [
        "### 4. Defining the training step to update parameters\n",
        "\n",
        "All of the steps above are combined into a training step that is run every episode. All steps leading up to the loss function are executed with the `tf.GradientTape` context to enable automatic differentiation.\n",
        "\n",
        "This tutorial uses the Adam optimizer to apply the gradients to the model parameters.\n",
        "\n",
        "The sum of the undiscounted rewards, `episode_reward`, is also computed in this step. This value will be used later on to evaluate if the success criterion is met.\n",
        "\n",
        "The `tf.function` context is applied to the `train_step` function so that it can be compiled into a callable TensorFlow graph, which can lead to 10x speedup in training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoccrkF3IFCg"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(\n",
        "    initial_state: tf.Tensor, \n",
        "    model: tf.keras.Model, \n",
        "    optimizer: tf.keras.optimizers.Optimizer, \n",
        "    gamma: float, \n",
        "    max_steps_per_episode: int) -> tf.Tensor:\n",
        "  \"\"\"Runs a model training step.\"\"\"\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # Run the model for one episode to collect training data\n",
        "    action_probs, values, rewards = run_episode(\n",
        "        initial_state, model, max_steps_per_episode) \n",
        "\n",
        "    # Calculate expected returns\n",
        "    returns = get_expected_return(rewards, gamma)\n",
        "\n",
        "    # Convert training data to appropriate TF tensor shapes\n",
        "    action_probs, values, returns = [\n",
        "        tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
        "\n",
        "    # Calculating loss values to update our network\n",
        "    loss = compute_loss(action_probs, values, returns)\n",
        "\n",
        "  # Compute the gradients from the loss\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "  # Apply the gradients to the model's parameters\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  episode_reward = tf.math.reduce_sum(rewards)\n",
        "\n",
        "  return episode_reward"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFvZiDoAflGK"
      },
      "source": [
        "### 5. Run the training loop\n",
        "\n",
        "Training is executed by running the training step until either the success criterion or maximum number of episodes is reached.  \n",
        "\n",
        "A running record of episode rewards is kept in a queue. Once 100 trials are reached, the oldest reward is removed at the left (tail) end of the queue and the newest one is added at the head (right). A running sum of the rewards is also maintained for computational efficiency. \n",
        "\n",
        "Depending on your runtime, training can finish in less than a minute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbmBxnzLiUJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c52c31b-d320-476c-c46b-054319349aad"
      },
      "source": [
        "%%time\n",
        "\n",
        "min_episodes_criterion = 100\n",
        "max_episodes = 10000\n",
        "max_steps_per_episode = 1000\n",
        "\n",
        "# Cartpole-v0 is considered solved if average reward is >= 195 over 100 \n",
        "# consecutive trials\n",
        "reward_threshold = 195\n",
        "running_reward = 0\n",
        "\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "\n",
        "# Keep last episodes reward\n",
        "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
        "\n",
        "with tqdm.trange(max_episodes) as t:\n",
        "  for i in t:\n",
        "    initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "    episode_reward = int(train_step(\n",
        "        initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
        "    \n",
        "    episodes_reward.append(episode_reward)\n",
        "    running_reward = statistics.mean(episodes_reward)\n",
        "  \n",
        "    t.set_description(f'Episode {i}')\n",
        "    t.set_postfix(\n",
        "        episode_reward=episode_reward, running_reward=running_reward)\n",
        "  \n",
        "    # Show average episode reward every 10 episodes\n",
        "    if i % 10 == 0:\n",
        "      pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
        "  \n",
        "    if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
        "        break\n",
        "\n",
        "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"while/actor_critic/dense/BiasAdd:0\", shape=(1, 6), dtype=float32) Tensor(\"while/actor_critic/dense_1/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
            "Tensor(\"while/actor_critic/dense/BiasAdd:0\", shape=(1, 6), dtype=float32) Tensor(\"while/actor_critic/dense_1/BiasAdd:0\", shape=(1, 1), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 44:   0%|          | 45/10000 [08:55<32:52:22, 11.89s/it, episode_reward=-18, running_reward=-18.8]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru8BEwS1EmAv"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "After training, it would be good to visualize how the model performs in the environment. You can run the cells below to generate a GIF animation of one episode run of the model. Note that additional packages need to be installed for OpenAI Gym to render the environment's images correctly in Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbIMMkfmRHyC"
      },
      "source": [
        "# Render an episode and save as a GIF file\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "\n",
        "def render_episode(env: gym.Env, model: tf.keras.Model, max_steps: int): \n",
        "  screen = env.render(mode='rgb_array')\n",
        "  im = Image.fromarray(screen)\n",
        "\n",
        "  images = [im]\n",
        "  \n",
        "  state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "  for i in range(1, max_steps + 1):\n",
        "    state = tf.expand_dims(state, 0)\n",
        "    action_probs, _ = model(state)\n",
        "    action = np.argmax(np.squeeze(action_probs))\n",
        "\n",
        "    state, _, done, _ = env.step(action)\n",
        "    state = tf.constant(state, dtype=tf.float32)\n",
        "\n",
        "    # Render screen every 10 steps\n",
        "    if i % 10 == 0:\n",
        "      screen = env.render(mode='rgb_array')\n",
        "      images.append(Image.fromarray(screen))\n",
        "  \n",
        "    if done:\n",
        "      break\n",
        "  \n",
        "  return images\n",
        "\n",
        "\n",
        "# Save GIF image\n",
        "images = render_episode(env, model, max_steps_per_episode)\n",
        "image_file = 'cartpole-v0.gif'\n",
        "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
        "images[0].save(\n",
        "    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLd720SejKmf"
      },
      "source": [
        "import tensorflow_docs.vis.embed as embed\n",
        "embed.embed_file(image_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnq9Hzo1Po6X"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "This tutorial demonstrated how to implement the actor-critic method using Tensorflow.\n",
        "\n",
        "As a next step, you could try training a model on a different environment in OpenAI Gym. \n",
        "\n",
        "For additional information regarding actor-critic methods and the Cartpole-v0 problem, you may refer to the following resources:\n",
        "\n",
        "- [Actor Critic Method](https://hal.inria.fr/hal-00840470/document)\n",
        "- [Actor Critic Lecture (CAL)](https://www.youtube.com/watch?v=EKqxumCuAAY&list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&index=7&t=0s)\n",
        "- [Cartpole learning control problem \\[Barto, et al. 1983\\]](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf) \n",
        "\n",
        "For more reinforcement learning examples in TensorFlow, you can check the following resources:\n",
        "- [Reinforcement learning code examples (keras.io)](https://keras.io/examples/rl/)\n",
        "- [TF-Agents reinforcement learning library](https://www.tensorflow.org/agents)\n"
      ]
    }
  ]
}